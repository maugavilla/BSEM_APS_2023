<!DOCTYPE html>
<html lang="en"><head>
<script src="BSEM_APS_2023_files/libs/clipboard/clipboard.min.js"></script>
<script src="BSEM_APS_2023_files/libs/quarto-html/tabby.min.js"></script>
<script src="BSEM_APS_2023_files/libs/quarto-html/popper.min.js"></script>
<script src="BSEM_APS_2023_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="BSEM_APS_2023_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="BSEM_APS_2023_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="BSEM_APS_2023_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="BSEM_APS_2023_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.353">

  <meta name="author" content="Mauricio Garnier-Villarreal">
  <meta name="dcterms.date" content="2024-04-09">
  <title>BSEM with blavaan</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="BSEM_APS_2023_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="BSEM_APS_2023_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #aaaaaa;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
    div.sourceCode
      { color: #003b4f; background-color: #f1f3f5; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #003b4f; } /* Normal */
    code span.al { color: #ad0000; } /* Alert */
    code span.an { color: #5e5e5e; } /* Annotation */
    code span.at { color: #657422; } /* Attribute */
    code span.bn { color: #ad0000; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #003b4f; } /* ControlFlow */
    code span.ch { color: #20794d; } /* Char */
    code span.cn { color: #8f5902; } /* Constant */
    code span.co { color: #5e5e5e; } /* Comment */
    code span.cv { color: #5e5e5e; font-style: italic; } /* CommentVar */
    code span.do { color: #5e5e5e; font-style: italic; } /* Documentation */
    code span.dt { color: #ad0000; } /* DataType */
    code span.dv { color: #ad0000; } /* DecVal */
    code span.er { color: #ad0000; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #ad0000; } /* Float */
    code span.fu { color: #4758ab; } /* Function */
    code span.im { color: #00769e; } /* Import */
    code span.in { color: #5e5e5e; } /* Information */
    code span.kw { color: #003b4f; } /* Keyword */
    code span.op { color: #5e5e5e; } /* Operator */
    code span.ot { color: #003b4f; } /* Other */
    code span.pp { color: #ad0000; } /* Preprocessor */
    code span.sc { color: #5e5e5e; } /* SpecialChar */
    code span.ss { color: #20794d; } /* SpecialString */
    code span.st { color: #20794d; } /* String */
    code span.va { color: #111111; } /* Variable */
    code span.vs { color: #20794d; } /* VerbatimString */
    code span.wa { color: #5e5e5e; font-style: italic; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="BSEM_APS_2023_files/libs/revealjs/dist/theme/quarto.css">
  <link href="BSEM_APS_2023_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="BSEM_APS_2023_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="BSEM_APS_2023_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="BSEM_APS_2023_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">BSEM with blavaan</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Mauricio Garnier-Villarreal <a href="https://orcid.org/0000-0002-2951-6647" class="quarto-title-author-orcid"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA2ZpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wTU09Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9tbS8iIHhtbG5zOnN0UmVmPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvc1R5cGUvUmVzb3VyY2VSZWYjIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iIHhtcE1NOk9yaWdpbmFsRG9jdW1lbnRJRD0ieG1wLmRpZDo1N0NEMjA4MDI1MjA2ODExOTk0QzkzNTEzRjZEQTg1NyIgeG1wTU06RG9jdW1lbnRJRD0ieG1wLmRpZDozM0NDOEJGNEZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wTU06SW5zdGFuY2VJRD0ieG1wLmlpZDozM0NDOEJGM0ZGNTcxMUUxODdBOEVCODg2RjdCQ0QwOSIgeG1wOkNyZWF0b3JUb29sPSJBZG9iZSBQaG90b3Nob3AgQ1M1IE1hY2ludG9zaCI+IDx4bXBNTTpEZXJpdmVkRnJvbSBzdFJlZjppbnN0YW5jZUlEPSJ4bXAuaWlkOkZDN0YxMTc0MDcyMDY4MTE5NUZFRDc5MUM2MUUwNEREIiBzdFJlZjpkb2N1bWVudElEPSJ4bXAuZGlkOjU3Q0QyMDgwMjUyMDY4MTE5OTRDOTM1MTNGNkRBODU3Ii8+IDwvcmRmOkRlc2NyaXB0aW9uPiA8L3JkZjpSREY+IDwveDp4bXBtZXRhPiA8P3hwYWNrZXQgZW5kPSJyIj8+84NovQAAAR1JREFUeNpiZEADy85ZJgCpeCB2QJM6AMQLo4yOL0AWZETSqACk1gOxAQN+cAGIA4EGPQBxmJA0nwdpjjQ8xqArmczw5tMHXAaALDgP1QMxAGqzAAPxQACqh4ER6uf5MBlkm0X4EGayMfMw/Pr7Bd2gRBZogMFBrv01hisv5jLsv9nLAPIOMnjy8RDDyYctyAbFM2EJbRQw+aAWw/LzVgx7b+cwCHKqMhjJFCBLOzAR6+lXX84xnHjYyqAo5IUizkRCwIENQQckGSDGY4TVgAPEaraQr2a4/24bSuoExcJCfAEJihXkWDj3ZAKy9EJGaEo8T0QSxkjSwORsCAuDQCD+QILmD1A9kECEZgxDaEZhICIzGcIyEyOl2RkgwAAhkmC+eAm0TAAAAABJRU5ErkJggg=="></a>
</div>
<div class="quarto-title-author-email">
<a href="mailto:m.garniervillarreal@vu.nl">m.garniervillarreal@vu.nl</a>
</div>
        <p class="quarto-title-affiliation">
            Vrije Universiteit Amsterdam
          </p>
    </div>
</div>

  <p class="date">April 9, 2024</p>
</section>
<section id="topics" class="slide level2 smaller">
<h2>Topics</h2>
<ul>
<li>Introduction to Bayesian probability</li>
<li>Evaluation of MCMC convergence and efficiency</li>
<li>Priors: selection and relevance</li>
<li>Prior predictive checks</li>
<li>BCFA: basic measurement model</li>
<li>Model fit evaluation</li>
<li>Model comparison</li>
<li>BSEM: basic latent regression</li>
<li>Probability of direction</li>
<li>Local fit</li>
<li>What to report</li>
</ul>
</section>
<section>
<section id="introduction-to-bayesian-probability" class="title-slide slide level1 smaller center">
<h1>Introduction to Bayesian probability</h1>

</section>
<section id="bayesian-data-analysis" class="slide level2">
<h2>Bayesian Data Analysis</h2>
<ul>
<li>Probability to describe uncertainty.</li>
<li>Extends discrete logic (true/false) to continuous plausibility.</li>
<li>Computationally difficult (MCMC). Wasn’t practical to use.</li>
<li>Based on Pierre-Simon Laplace and Thomas Bayes. Older than frequentist.</li>
<li>Used to be controversial (still?? maybe depends of the field??) .</li>
</ul>
</section>
<section id="bayesian-data-analysis-1" class="slide level2">
<h2>Bayesian Data Analysis</h2>
<ul>
<li>Frequentist view.
<ul>
<li>Probability is just limiting frequency.</li>
<li>Uncertainty arises from sampling variation.</li>
</ul></li>
<li>Bayesian view (more general).
<ul>
<li>Probability is part of the models.</li>
<li>Uncertainty is due to how much we don’t know: How much the model doesn’t know.</li>
</ul></li>
</ul>
</section>
<section id="logic-example" class="slide level2">
<h2>Logic example</h2>
<ul>
<li>WLWWWLWLW</li>
</ul>

<img data-src="world.png" class="r-stretch quarto-figure-center"><p class="caption">World</p></section>
<section id="design-the-model" class="slide level2">
<h2>Design the model</h2>
<ul>
<li><p>What generates the data?</p></li>
<li><p>For WLWWWLWLW.</p>
<ul>
<li>Some true proportion of water p</li>
<li>Toss globe, probability <span class="math inline">\(p\)</span> of observing <span class="math inline">\(W\)</span>, <span class="math inline">\(1-p\)</span> of</li>
<li>Independent tosses.</li>
</ul></li>
<li><p>Probability statement.</p></li>
</ul>

<img data-src="world.png" class="r-stretch quarto-figure-center"><p class="caption">World</p></section>
<section id="condition-on-the-data" class="slide level2">
<h2>Condition on the data</h2>
<ul>
<li>Condition the model on the data.</li>
<li>Update the prior with the data <span class="math inline">\(\rightarrow\)</span> posterior.</li>
<li>The information is updated at each step, model is informed by the model characteristics and data.</li>
</ul>
</section>
<section id="starting-flat" class="slide level2">
<h2>Starting flat</h2>

<img data-src="fig1.png" class="r-stretch quarto-figure-center"><p class="caption">Flat</p></section>
<section id="update" class="slide level2">
<h2>Update</h2>
<ul>
<li>Observe = W</li>
</ul>

<img data-src="fig2.png" class="r-stretch quarto-figure-center"><p class="caption">Update</p></section>
<section id="wlwwwlwlw" class="slide level2">
<h2>WLWWWLWLW</h2>

<img data-src="fig3.png" class="r-stretch quarto-figure-center"><p class="caption">Update</p></section>
<section id="condition-on-data" class="slide level2">
<h2>Condition on data</h2>
<ul>
<li>Tosses are independent: order of data is irrelevant.</li>
<li>Every posterior is a prior for next observation.</li>
<li>Every prior is a posterior of some other inference.</li>
</ul>
</section>
<section id="evaluate-the-model" class="slide level2">
<h2>Evaluate the model</h2>
<ul>
<li>Bayesian inference: logical answer to a question.</li>
<li>Answers are in form of distributions.</li>
<li>You guide the model.
<ul>
<li>Was there a problem.</li>
<li>Makes sense.</li>
<li>Sensitivity.</li>
</ul></li>
</ul>
</section>
<section id="bayesian-model" class="slide level2">
<h2>Bayesian Model</h2>
<ul>
<li>Assume:
<ul>
<li>Likelihood.</li>
<li>Parameters.</li>
<li>Priors.</li>
</ul></li>
<li>Produce: Posterior.</li>
</ul>
</section>
<section id="likelihood" class="slide level2">
<h2>Likelihood</h2>
<ul>
<li><span class="math inline">\(Pr(data|assumptions)\)</span>
<ul>
<li>Probability of observations conditional on assumptions/model.</li>
<li>Mathematical form of how the data happens.</li>
</ul></li>
<li>In frequentist: <span class="math inline">\(Pr(data|Ho)\)</span>
<ul>
<li>Probability of the data if the null hypothesis is true.</li>
</ul></li>
<li>In the globe example: binomial probability:
<ul>
<li>Probability of getting a 1 in a toss: coin, globe, etc.</li>
</ul></li>
</ul>
</section>
<section id="parameters" class="slide level2">
<h2>Parameters</h2>
<ul>
<li>Parameters that define the probability function of the likelihood.</li>
<li>What parameters define the distribution that you specify for the data.</li>
<li>Depends of the likelihood function:
<ul>
<li>Normal: mean, sd.</li>
<li>Binomial: <span class="math inline">\(p\)</span></li>
</ul></li>
</ul>
</section>
<section id="prior" class="slide level2">
<h2>Prior</h2>
<ul>
<li>Original believe/knowledge/information for the parameters.</li>
<li>Define as distribution.</li>
<li>You always know “something”.
<ul>
<li>Globe example: uniform</li>
</ul></li>
</ul>

<img data-src="prior.png" class="r-stretch quarto-figure-center"><p class="caption">prior</p></section>
<section id="prior-1" class="slide level2">
<h2>Prior</h2>
<ul>
<li><span class="math inline">\(P(\theta)\)</span> is the prior distribution represents some prior belief or information (without seeing data) about the distribution of <span class="math inline">\(\theta\)</span> .</li>
<li>By specifying a density function we expect <span class="math inline">\(\theta\)</span> to follow, we can then estimate the form of the posterior for parameters.</li>
</ul>
</section>
<section id="posterior-and-bayes-ruletheorem" class="slide level2 smaller">
<h2>Posterior and Bayes Rule/Theorem</h2>
<ul>
<li>Bayesian estimate is a posterior distribution over parameters <span class="math inline">\(Pr(parameters|data)\)</span>.</li>
<li>We can solve for the posterior distribution <span class="math inline">\(Pr(\theta|y)\)</span>, represents the probability for our parameter(<span class="math inline">\(s\)</span>) of interest (<span class="math inline">\(\theta\)</span>), given data (<span class="math inline">\(y\)</span>)</li>
</ul>
<p><span class="math display">\[
p(\theta|y) = \frac{p(\theta,y)}{p(y)} = \frac{p(y|\theta)p(\theta)}{p(y)}
\]</span></p>
<p><span class="math display">\[
p(\theta|y) \propto p(y|\theta)p(\theta)
\]</span></p>
</section>
<section id="posterior" class="slide level2">
<h2>Posterior</h2>
<ul>
<li>We describe the distribution: point estimate, sd, intervals, etc.</li>
<li>Posterior quantifies the uncertainty about <span class="math inline">\(\theta\)</span>, conditional on data.</li>
<li>You decide how you describe it, what is meaningful for your research question.</li>
</ul>
</section>
<section id="p-value" class="slide level2">
<h2>p-value</h2>
<ul>
<li><span class="math inline">\(Pr(y|\theta) = P(y &gt; Y|H_{0})\)</span>.</li>
<li>Probability of the data coming from a population where the Null Hypothesis is TRUE.</li>
<li>Probability of observing data (<span class="math inline">\(y\)</span>) past a threshold (<span class="math inline">\(Y\)</span>), given a null hypothesis is true.</li>
<li>Major problems: 1) people misinterpret this ALL the time, 2) it is not the inference you really want.</li>
</ul>
</section>
<section id="the-tyranny-of-the-p-value" class="slide level2">
<h2>The tyranny of the <span class="math inline">\(p\)</span>-value</h2>
<ul>
<li>People frequently confuse <span class="math inline">\(Pr(y &gt; Y|H_{0})\)</span> with <span class="math inline">\(Pr(H0|y &gt; Y)\)</span>. If the probability of the data, given the null is true, is small, the probability that the null is true, given the data, must be small, too, right?!RIGHT?! Sadly NO.</li>
<li>With the Bayes rule, we know they are only equal if the marginal probability of H0 being true is equal to the marginal probability of data being greater than or equal to the threshold.
<ul>
<li>There is no reason to think that is the case.</li>
</ul></li>
</ul>
</section>
<section id="frequentists-vs.-bayesians" class="slide level2">
<h2>Frequentists vs.&nbsp;Bayesians</h2>
<ul>
<li><p>Frequentist “What is the likelihood of observing these data, given the parameter(s) of the model?” Maximum likelihood methods basically work by iteratively finding values for q that maximize this function.</p></li>
<li><p>Bayesian “What is the distribution of the parameters, given the data?” A Bayesian is interested in how the parameters can be inferred from the data, not how the data would have been inferred from the parameters.</p></li>
</ul>
</section>
<section id="the-p-you-really-want-to-know" class="slide level2">
<h2>The “P” you really want to know</h2>
<ul>
<li>We will not be rejecting any null hypotheses in here. We will make direct probabilistic inferences about the values of our parameters of interest. A Bayesian can always express the probability that (for example) a mean difference is greater than zero, if desired. But what’s almost certainly more interesting is the inference about how large the mean difference between the groups really is!</li>
</ul>
</section></section>
<section>
<section id="convergence-and-efficiency-evaluation" class="title-slide slide level1 center">
<h1>Convergence and Efficiency Evaluation</h1>

</section>
<section id="terms" class="slide level2">
<h2>Terms</h2>
<ul>
<li>Iterations: number of times we want the MCMC algorithm to run (estimate)
<ul>
<li>Burnin: number of iteration to use to calibrate the model find a stable solution</li>
<li>Sample: number of iterations to save after burnin, to build the posterior distributions</li>
</ul></li>
<li>Chains: number of times we estimate models N-iterations, with different starting values</li>
<li>Thin: number of sample iterations to skip over (only recommended to save memory space)</li>
</ul>
</section>
<section id="convergence" class="slide level2 smaller">
<h2>Convergence</h2>
<ul>
<li>When Bayesian models estimated with Markov-Chain Monte Carlo (MCMC) sampler, the models dont stop when it has achieve some convergence criteria, it will run as long as you set it to, and then you need to evaluate the convergence and efficiency of the estimated posterior distributions. And only analyze the results if they are stable enough.</li>
<li><span class="math inline">\(\hat{R}\)</span> is the convergence diagnostic, which compares the between- and within-chain estimates for model parameters and other univariate quantities of interest <span class="citation" data-cites="new_rhat">(<a href="#/references" role="doc-biblioref" onclick="">Vehtari et al. 2021</a>)</span>.</li>
<li>If chains have not mixed well (ie, the between- and within-chain estimates don’t agree), <span class="math inline">\(\hat{R}\)</span> is larger than 1. We recommend running at least three chains by default and only using the sample if <span class="math inline">\(\hat{R} &lt; 1.05\)</span> for all the parameters.</li>
</ul>
</section>
<section id="convergence-1" class="slide level2">
<h2>Convergence</h2>
<ul>
<li>If all <span class="math inline">\(\hat{R} &lt; 1.05\)</span> then we can establish that the MCMC chains have converged to a stable solution. If the model has not converged, you should increase the number of <code>burnin</code> iterations</li>
<li>and/or change the model priors. As the model might have failed to converge due to needing more iterations or a model misspecification (such as bad priors)</li>
</ul>
</section>
<section id="convergence-2" class="slide level2">
<h2>Convergence</h2>

<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-3-1.png" width="960" class="r-stretch"></section>
<section id="convergence-3" class="slide level2">
<h2>Convergence</h2>

<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-5-1.png" width="960" class="r-stretch"></section>
<section id="efficiency" class="slide level2 smaller">
<h2>Efficiency</h2>
<ul>
<li>Effective sample size (ESS) measures sampling efficiency in the distribution (related e.g.&nbsp;to efficiency of mean and median estimates), and is well defined even if the chains do not have finite mean or variance <span class="citation" data-cites="new_rhat">(<a href="#/references" role="doc-biblioref" onclick="">Vehtari et al. 2021</a>)</span>.</li>
<li>ESS can be interpreted as the number of posterior draws that are completely independent of each other, with auto-correlations of 0</li>
<li>ESS should be at least 100 (approximately) per Markov Chain in order to be reliable and indicate that estimates of respective posterior quantiles are reliable, e.g.: <span class="math inline">\(ESS &gt; 300\)</span> with 3 chains for every parameter</li>
</ul>
</section></section>
<section>
<section id="priors-selection-and-relevance" class="title-slide slide level1 center">
<h1>Priors: selection and relevance</h1>

</section>
<section id="priors" class="slide level2">
<h2>Priors</h2>
<ul>
<li><span class="math inline">\(p(\theta)\)</span> is the “prior distribution”</li>
<li>Represents your knowledge and level of uncertainty</li>
<li>Represented as probability distributions</li>
<li>The inclusion of priors is a strength not a weakness.</li>
<li>Bayesian inference can implement cumulative scientific progress with the inclusion of previous knowledge into the specification of the prior uncertainty</li>
</ul>
</section>
<section id="sample-size" class="slide level2">
<h2>Sample size</h2>
<ul>
<li>Frequentist statistics are asymptotically correct</li>
<li>Bayesian is estimate in function the know data</li>
<li>Small samples have a better representation with Bayesian statistics.</li>
<li>It does not mean is perfect, you are still limited by your data</li>
</ul>
</section>
<section id="prior-advantages" class="slide level2">
<h2>Prior: advantages</h2>
<ul>
<li>Include prior knowledge</li>
<li>Account for uncertainty</li>
<li>Allow us to set clear boundaries, meaningful for the theory</li>
<li>Theory driven</li>
<li>Helps stabilize models with smaller sample sizes</li>
</ul>
</section>
<section id="prior-disadvantages" class="slide level2">
<h2>Prior: disadvantages</h2>
<ul>
<li>More decisions to make</li>
<li>Can bias the results if they are strong in the wrong place</li>
<li>Bad priors can make the model take longer to converge</li>
<li>More effect with smaller sample sizes</li>
</ul>
</section>
<section id="priors-1" class="slide level2">
<h2>Priors</h2>
<ul>
<li><p>Non informative (diffuse)</p></li>
<li><p>Weakly informative</p></li>
<li><p>Strongly informative</p></li>
<li><p>The different types relate to the amount of uncertainty</p></li>
<li><p>The recommended standard one is weakly informative</p></li>
<li><p>Apologetic Bayesian prefer non informative</p></li>
</ul>
</section>
<section id="non-informative-priors" class="slide level2">
<h2>Non informative Priors</h2>
<ul>
<li>Intend to have large variances, implying large uncertainty</li>
<li>Telling the model that you have no notion of where the parameters are located</li>
<li>Try to be as similar as possible to ML, since in ML every parameter value is possible</li>
<li>Even if the parameters are equal to ML, the inference is never the same</li>
<li><span class="math inline">\(p(\theta) \sim N(0, 100000)\)</span></li>
<li><span class="math inline">\(p(\theta) \sim U(-10000, 10000)\)</span></li>
</ul>
</section>
<section id="non-informative-priors-1" class="slide level2">
<h2>Non informative Priors</h2>
<ul>
<li>Even as they are called “non informative”</li>
<li>It is believed that if the prior tells the model that many values are possible, then it is not providing information</li>
<li>Actually, it is providing a lot of information, bad information, telling the model that outlier values are possible</li>
<li>Better to called them “diffuse” for the lack of clarity and quality of the information</li>
</ul>
</section>
<section id="weakly-informative-priors" class="slide level2">
<h2>Weakly informative Priors</h2>
<ul>
<li>Represents a reasonable level of uncertainty</li>
<li>It does not intend to drive the parameters/posterior</li>
<li>Intends to set a reasonable parameter space (boundaries)</li>
<li>Theory/data driven</li>
<li><span class="math inline">\(p(\theta) \sim N(0, 10)\)</span></li>
<li><span class="math inline">\(p(\theta) \sim U(0, 100)\)</span></li>
</ul>
</section>
<section id="strongly-informative-priors" class="slide level2">
<h2>Strongly informative Priors</h2>
<ul>
<li>Represents a low level of uncertainty</li>
<li>Usually use to present specific hypothesis</li>
<li>Not recommended for general use in parameters</li>
<li><span class="math inline">\(p(\theta) \sim N(0, .05)\)</span></li>
<li><span class="math inline">\(p(\theta) \sim U(0, 1)\)</span></li>
</ul>
</section>
<section id="priors-2" class="slide level2">
<h2>Priors</h2>
<ul>
<li>Have more influence on the posterior for smaller samples</li>
<li>Consider theory, data, and model characteristics</li>
<li>Are scale dependent, what is a weakly informative prior in one case might be strong in another</li>
<li>The “intended” priors might differ from the priors in the model due to model constraints, as opaque priors <span class="citation" data-cites="merkle2023opaque">(<a href="#/references" role="doc-biblioref" onclick="">Merkle et al. 2023</a>)</span></li>
</ul>
</section></section>
<section>
<section id="prior-predictive-checks" class="title-slide slide level1 center">
<h1>Prior predictive checks</h1>

</section>
<section id="prior-predictive-checks-ppc" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<ul>
<li>Generate data from the priors in order to assess whether a prior is appropriate <span class="citation" data-cites="Gabry_2019_vis">(<a href="#/references" role="doc-biblioref" onclick="">Gabry et al. 2019</a>)</span>.</li>
<li>A posterior predictive check generates replicated data according to the posterior predictive distribution.</li>
<li>In contrast, the prior predictive check generates data according to the prior predictive distribution</li>
</ul>
<p><span class="math inline">\(y^{sim} ∼ p(y)\)</span></p>
</section>
<section id="prior-predictive-checks-ppc-1" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<ul>
<li>Like the posterior predictive distribution with no observed data, so that a PPC is nothing more than the limiting case of a posterior predictive check with no data.</li>
<li>Simulating parameters <span class="math inline">\(θ^{sim}∼p(\theta)\)</span> according to the priors, then simulating data <span class="math inline">\(y^{sim}∼p(y∣ \theta^{sim})\)</span> according to the sampling distribution given the simulated parameters</li>
<li>The result is a simulation from the joint distribution, <span class="math inline">\((y^{sim},θ^{sim})∼p(y,\theta)\)</span> and thus <span class="math inline">\(y^{sim}∼p(y)\)</span> is a simulation from the prior predictive distribution.</li>
</ul>
</section>
<section id="prior-predictive-checks-ppc-2" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-7_7410a700897de90829f5ba7a4bf76da6">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1"></a>priors <span class="ot">&lt;-</span> <span class="fu">dpriors</span>(<span class="at">nu=</span><span class="st">"normal(3,2)"</span>,</span>
<span id="cb1-2"><a href="#cb1-2"></a>                  <span class="at">lambda=</span><span class="st">"normal(0.4, 2)"</span>,</span>
<span id="cb1-3"><a href="#cb1-3"></a>                  <span class="at">beta=</span><span class="st">"normal(0.4, 2)"</span>,</span>
<span id="cb1-4"><a href="#cb1-4"></a>                  <span class="at">theta=</span><span class="st">"gamma(1,1)[sd]"</span>)</span>
<span id="cb1-5"><a href="#cb1-5"></a></span>
<span id="cb1-6"><a href="#cb1-6"></a>model <span class="ot">&lt;-</span> <span class="st">'</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="st">  # latent variable definitions</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="st">     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="st">     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8</span></span>
<span id="cb1-11"><a href="#cb1-11"></a></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="st">  # regressions</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="st">    dem60 ~ ind60</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="st">    dem65 ~ ind60 + dem60</span></span>
<span id="cb1-15"><a href="#cb1-15"></a></span>
<span id="cb1-16"><a href="#cb1-16"></a><span class="st">  # residual correlations</span></span>
<span id="cb1-17"><a href="#cb1-17"></a><span class="st">    y1 ~~ y5</span></span>
<span id="cb1-18"><a href="#cb1-18"></a><span class="st">    y2 ~~ y4 + y6</span></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="st">    y3 ~~ y7</span></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="st">    y4 ~~ y8</span></span>
<span id="cb1-21"><a href="#cb1-21"></a><span class="st">    y6 ~~ y8</span></span>
<span id="cb1-22"><a href="#cb1-22"></a><span class="st">'</span></span>
<span id="cb1-23"><a href="#cb1-23"></a></span>
<span id="cb1-24"><a href="#cb1-24"></a>fit_wi <span class="ot">&lt;-</span> <span class="fu">bsem</span>(model, <span class="at">data=</span>PoliticalDemocracy, <span class="at">std.lv=</span>T,</span>
<span id="cb1-25"><a href="#cb1-25"></a>            <span class="at">meanstructure=</span>T, <span class="at">test =</span> <span class="st">"none"</span>,</span>
<span id="cb1-26"><a href="#cb1-26"></a>            <span class="at">dp=</span>priors, <span class="at">prisamp =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="prior-predictive-checks-ppc-3" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-8_05a53ed150dd49c655176ec5c40c82c7">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1"></a><span class="do">## factor loadings</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="fu">plot</span>(fit_wi, <span class="at">pars=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">11</span>, <span class="at">plot.type =</span> <span class="st">"dens"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-8-1.png" width="960" class="r-stretch"></section>
<section id="prior-predictive-checks-ppc-4" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-9_93680fc92db34d9b15fd71355c8e4b77">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1"></a><span class="do">## factor regressions</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="fu">plot</span>(fit_wi, <span class="at">pars=</span><span class="dv">12</span><span class="sc">:</span><span class="dv">14</span>, <span class="at">plot.type =</span> <span class="st">"dens"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-9-1.png" width="960" class="r-stretch"></section>
<section id="prior-predictive-checks-ppc-5" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-10_e6d99dce903f85849896810a5256265e">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1"></a><span class="do">## residual variances</span></span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="fu">plot</span>(fit_wi, <span class="at">pars=</span><span class="dv">15</span><span class="sc">:</span><span class="dv">31</span>, <span class="at">plot.type =</span> <span class="st">"dens"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-10-1.png" width="960" class="r-stretch"></section>
<section id="prior-predictive-checks-ppc-6" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-11_09f0a310f47b6e38dadb8fced940b3fd">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1"></a><span class="do">## item intercepts</span></span>
<span id="cb5-2"><a href="#cb5-2"></a><span class="fu">plot</span>(fit_wi, <span class="at">pars=</span><span class="dv">32</span><span class="sc">:</span><span class="dv">42</span>, <span class="at">plot.type =</span> <span class="st">"dens"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-11-1.png" width="960" class="r-stretch"></section>
<section id="prior-predictive-checks-ppc-7" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<ul>
<li>Default priors</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-12_cc5c67d7c7ff273f4def288e53d67bc7">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1"></a><span class="fu">dpriors</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>               nu             alpha            lambda              beta 
   "normal(0,32)"    "normal(0,10)"    "normal(0,10)"    "normal(0,10)" 
            theta               psi               rho             ibpsi 
"gamma(1,.5)[sd]" "gamma(1,.5)[sd]"       "beta(1,1)" "wishart(3,iden)" 
              tau 
  "normal(0,1.5)" </code></pre>
</div>
</div>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-14_40ddca3609aa959d965f316582f6136f">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1"></a>fit_df <span class="ot">&lt;-</span> <span class="fu">bsem</span>(model, <span class="at">data=</span>PoliticalDemocracy, <span class="at">std.lv=</span>T,</span>
<span id="cb8-2"><a href="#cb8-2"></a>            <span class="at">meanstructure=</span>T, <span class="at">test =</span> <span class="st">"none"</span>,</span>
<span id="cb8-3"><a href="#cb8-3"></a>            <span class="at">prisamp =</span> T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="prior-predictive-checks-ppc-8" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-15_ae90b893adc36c259418f591def84c7a">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1"></a><span class="do">## factor loadings</span></span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="fu">plot</span>(fit_df, <span class="at">pars=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">11</span>, <span class="at">plot.type =</span> <span class="st">"dens"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-15-1.png" width="960" class="r-stretch"></section>
<section id="prior-predictive-checks-ppc-9" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-16_62504fec39158fa98619a45edf079533">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1"></a><span class="do">## factor regressions</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="fu">plot</span>(fit_df, <span class="at">pars=</span><span class="dv">12</span><span class="sc">:</span><span class="dv">14</span>, <span class="at">plot.type =</span> <span class="st">"dens"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-16-1.png" width="960" class="r-stretch"></section>
<section id="prior-predictive-checks-ppc-10" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-17_4338c271df962d3f61b9a871fabfe9e0">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1"></a><span class="do">## residual variances</span></span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="fu">plot</span>(fit_df, <span class="at">pars=</span><span class="dv">15</span><span class="sc">:</span><span class="dv">31</span>, <span class="at">plot.type =</span> <span class="st">"dens"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-17-1.png" width="960" class="r-stretch"></section>
<section id="prior-predictive-checks-ppc-11" class="slide level2">
<h2>Prior predictive checks (PPC)</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-18_08c89129513d020c50a13635d496df80">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1"></a><span class="do">## item intercepts</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="fu">plot</span>(fit_df, <span class="at">pars=</span><span class="dv">32</span><span class="sc">:</span><span class="dv">42</span>, <span class="at">plot.type =</span> <span class="st">"dens"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-18-1.png" width="960" class="r-stretch"></section></section>
<section>
<section id="bcfa-basic-measurement-model" class="title-slide slide level1 center">
<h1>BCFA: basic measurement model</h1>

</section>
<section id="cfa-measurement-models" class="slide level2">
<h2>CFA: measurement models</h2>
<ul>
<li>A construct is what the indicators share</li>
</ul>

<img data-src="cfa1.png" class="r-stretch"></section>
<section id="bayesian-cfa" class="slide level2">
<h2>Bayesian CFA</h2>
<p>For this example we will use the Industrialization and Political Democracy example <span class="citation" data-cites="bollen_structural_1989">(<a href="#/references" role="doc-biblioref" onclick="">Bollen 1989</a>)</span></p>

<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-20-1.png" width="960" class="r-stretch"></section>
<section id="bayesian-cfa-1" class="slide level2">
<h2>Bayesian CFA</h2>

<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-22-1.png" width="960" class="r-stretch"></section>
<section id="measurement-models" class="slide level2">
<h2>Measurement models</h2>
<ul>
<li>A model is fitted to data and all models are wrong to some degree, the data may not be explained perfectly</li>
<li>Interpretations must involve a subjective component and solutions will not make sense</li>
<li>Model fit should be evaluated</li>
<li>Tests theoretical structure</li>
</ul>
</section>
<section id="bayesian-cfa-2" class="slide level2">
<h2>Bayesian CFA</h2>
<ul>
<li>Basic measurement model (default priors)</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-23_1ec1b6eabbe5421f301f728d580d41f4">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1"></a>mod1 <span class="ot">&lt;-</span> <span class="st">'</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="st">  # latent variable definitions</span></span>
<span id="cb13-3"><a href="#cb13-3"></a><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span id="cb13-4"><a href="#cb13-4"></a><span class="st">     dem60 =~ y1 + y2 + y3 + y4</span></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="st">     dem65 =~ y5 + y6 + y7 + y8'</span></span>
<span id="cb13-6"><a href="#cb13-6"></a></span>
<span id="cb13-7"><a href="#cb13-7"></a>f1 <span class="ot">&lt;-</span> <span class="fu">bcfa</span>(mod1, <span class="at">data=</span>PoliticalDemocracy, </span>
<span id="cb13-8"><a href="#cb13-8"></a>           <span class="at">meanstructure=</span>T, <span class="at">std.lv=</span>T,</span>
<span id="cb13-9"><a href="#cb13-9"></a>           <span class="at">burnin=</span><span class="dv">1000</span>, <span class="at">sample=</span><span class="dv">1000</span>, <span class="at">n.chains=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.00061 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 6.1 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 4.683 seconds (Warm-up)
Chain 1:                4.145 seconds (Sampling)
Chain 1:                8.828 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000227 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.27 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 4.818 seconds (Warm-up)
Chain 2:                4.018 seconds (Sampling)
Chain 2:                8.836 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000209 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.09 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 5.368 seconds (Warm-up)
Chain 3:                4.435 seconds (Sampling)
Chain 3:                9.803 seconds (Total)
Chain 3: 
Computing post-estimation metrics (including lvs if requested)...</code></pre>
</div>
</div>
</section>
<section id="convergence-and-efficiency" class="slide level2">
<h2>Convergence and efficiency</h2>
<ul>
<li>Convergence</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-24_5444d8f37a4f9d2837796ebd68f18d51">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1"></a><span class="fu">max</span>(<span class="fu">blavInspect</span>(f1, <span class="st">"psrf"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.004735</code></pre>
</div>
</div>
<ul>
<li>Efficiency</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-25_30c0a261b9b998ce5b3226d3c70a55e9">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1"></a><span class="fu">min</span>(<span class="fu">blavInspect</span>(f1, <span class="st">"neff"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1032.675</code></pre>
</div>
</div>
</section>
<section id="parameter-posteriors" class="slide level2">
<h2>Parameter posteriors</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-26_c59c58e3e77f70fa75e905abe21c996a">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1"></a><span class="fu">summary</span>(f1, <span class="at">rsquare=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>blavaan 0.5.3 ended normally after 1000 iterations

  Estimator                                      BAYES
  Optimization method                             MCMC
  Number of model parameters                        36

  Number of observations                            75

  Statistic                                 MargLogLik         PPP
  Value                                      -1697.089       0.036

Parameter Estimates:


Latent Variables:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       
  ind60 =~                                                                     
    x1                0.701    0.072    0.571    0.859    1.000    normal(0,10)
    x2                1.536    0.140    1.284    1.832    0.999    normal(0,10)
    x3                1.273    0.139    1.012    1.552    0.999    normal(0,10)
  dem60 =~                                                                     
    y1                2.316    0.279    1.810    2.891    1.002    normal(0,10)
    y2                3.146    0.431    2.360    4.019    1.001    normal(0,10)
    y3                2.429    0.369    1.728    3.182    1.002    normal(0,10)
    y4                3.029    0.351    2.369    3.731    1.000    normal(0,10)
  dem65 =~                                                                     
    y5                2.178    0.280    1.655    2.760    1.001    normal(0,10)
    y6                2.775    0.364    2.107    3.553    1.001    normal(0,10)
    y7                2.817    0.339    2.196    3.544    1.001    normal(0,10)
    y8                2.900    0.333    2.313    3.626    1.002    normal(0,10)

Covariances:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       
  ind60 ~~                                                                     
    dem60             0.432    0.107    0.214    0.620    1.000     lkj_corr(1)
    dem65             0.534    0.095    0.335    0.706    1.000     lkj_corr(1)
  dem60 ~~                                                                     
    dem65             0.954    0.028    0.887    0.993    0.999     lkj_corr(1)

Intercepts:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       
   .x1                5.051    0.089    4.876    5.224    1.000    normal(0,32)
   .x2                4.787    0.181    4.425    5.145    1.001    normal(0,32)
   .x3                3.554    0.169    3.213    3.879    1.001    normal(0,32)
   .y1                5.460    0.308    4.849    6.081    1.002    normal(0,32)
   .y2                4.244    0.462    3.330    5.149    1.003    normal(0,32)
   .y3                6.558    0.391    5.782    7.347    1.005    normal(0,32)
   .y4                4.438    0.399    3.667    5.238    1.004    normal(0,32)
   .y5                5.130    0.308    4.522    5.732    1.003    normal(0,32)
   .y6                2.968    0.392    2.178    3.736    1.003    normal(0,32)
   .y7                6.184    0.384    5.434    6.943    1.002    normal(0,32)
   .y8                4.027    0.383    3.281    4.785    1.002    normal(0,32)
    ind60             0.000                                                    
    dem60             0.000                                                    
    dem65             0.000                                                    

Variances:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       
   .x1                0.091    0.023    0.049    0.139    1.001 gamma(1,.5)[sd]
   .x2                0.117    0.080    0.001    0.296    1.000 gamma(1,.5)[sd]
   .x3                0.513    0.104    0.340    0.745    0.999 gamma(1,.5)[sd]
   .y1                2.086    0.472    1.286    3.103    1.000 gamma(1,.5)[sd]
   .y2                6.821    1.312    4.659    9.900    1.000 gamma(1,.5)[sd]
   .y3                5.595    1.048    3.828    7.898    1.000 gamma(1,.5)[sd]
   .y4                2.964    0.719    1.754    4.529    1.001 gamma(1,.5)[sd]
   .y5                2.595    0.527    1.724    3.765    1.000 gamma(1,.5)[sd]
   .y6                4.491    0.866    3.032    6.404    1.000 gamma(1,.5)[sd]
   .y7                3.696    0.746    2.429    5.373    1.000 gamma(1,.5)[sd]
   .y8                2.991    0.667    1.839    4.484    1.000 gamma(1,.5)[sd]
    ind60             1.000                                                    
    dem60             1.000                                                    
    dem65             1.000                                                    

R-Square:
                   Estimate
    x1                0.844
    x2                0.953
    x3                0.759
    y1                0.720
    y2                0.592
    y3                0.513
    y4                0.756
    y5                0.646
    y6                0.632
    y7                0.682
    y8                0.738</code></pre>
</div>
</div>
</section>
<section id="bayesian-cfa-3" class="slide level2">
<h2>Bayesian CFA</h2>
<ul>
<li>Basic measurement model (weakly informative priors)</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-27_966ac29b3671d18025692b3109496593">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1"></a>priors <span class="ot">&lt;-</span> <span class="fu">dpriors</span>(<span class="at">nu=</span><span class="st">"normal(3,2)"</span>,</span>
<span id="cb21-2"><a href="#cb21-2"></a>                  <span class="at">lambda=</span><span class="st">"normal(1, 3)"</span>,</span>
<span id="cb21-3"><a href="#cb21-3"></a>                  <span class="at">theta=</span><span class="st">"gamma(1,1)[sd]"</span>)</span>
<span id="cb21-4"><a href="#cb21-4"></a></span>
<span id="cb21-5"><a href="#cb21-5"></a>mod1 <span class="ot">&lt;-</span> <span class="st">'</span></span>
<span id="cb21-6"><a href="#cb21-6"></a><span class="st">  # latent variable definitions</span></span>
<span id="cb21-7"><a href="#cb21-7"></a><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span id="cb21-8"><a href="#cb21-8"></a><span class="st">     dem60 =~ y1 + y2 + y3 + y4</span></span>
<span id="cb21-9"><a href="#cb21-9"></a><span class="st">     dem65 =~ y5 + y6 + y7 + y8'</span></span>
<span id="cb21-10"><a href="#cb21-10"></a></span>
<span id="cb21-11"><a href="#cb21-11"></a>f2 <span class="ot">&lt;-</span> <span class="fu">bcfa</span>(mod1, <span class="at">data=</span>PoliticalDemocracy, </span>
<span id="cb21-12"><a href="#cb21-12"></a>           <span class="at">meanstructure=</span>T, <span class="at">std.lv=</span>T, <span class="at">dp=</span>priors,</span>
<span id="cb21-13"><a href="#cb21-13"></a>           <span class="at">burnin=</span><span class="dv">1000</span>, <span class="at">sample=</span><span class="dv">1000</span>, <span class="at">n.chains=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000208 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.08 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 5.153 seconds (Warm-up)
Chain 1:                3.813 seconds (Sampling)
Chain 1:                8.966 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000229 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.29 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 4.633 seconds (Warm-up)
Chain 2:                3.744 seconds (Sampling)
Chain 2:                8.377 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000211 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.11 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 4.613 seconds (Warm-up)
Chain 3:                4.754 seconds (Sampling)
Chain 3:                9.367 seconds (Total)
Chain 3: 
Computing post-estimation metrics (including lvs if requested)...</code></pre>
</div>
</div>
</section>
<section id="convergence-and-efficiency-1" class="slide level2">
<h2>Convergence and efficiency</h2>
<ul>
<li>Convergence</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-28_23637f042e24aad63b97c8ef007a53cf">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1"></a><span class="fu">max</span>(<span class="fu">blavInspect</span>(f2, <span class="st">"psrf"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.002336</code></pre>
</div>
</div>
<ul>
<li>Efficiency</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-29_da3b9385d28df975255b84b2d437403a">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1"></a><span class="fu">min</span>(<span class="fu">blavInspect</span>(f2, <span class="st">"neff"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 963.43</code></pre>
</div>
</div>
</section>
<section id="parameter-posteriors-1" class="slide level2">
<h2>Parameter posteriors</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-30_a167c3ff2b6b52d4bdcc9b8030f67b0a">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1"></a><span class="fu">summary</span>(f2, <span class="at">rsquare=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>blavaan 0.5.3 ended normally after 1000 iterations

  Estimator                                      BAYES
  Optimization method                             MCMC
  Number of model parameters                        36

  Number of observations                            75

  Statistic                                 MargLogLik         PPP
  Value                                      -1667.792       0.040

Parameter Estimates:


Latent Variables:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior      
  ind60 =~                                                                    
    x1                0.698    0.070    0.572    0.844    1.000   normal(1, 3)
    x2                1.532    0.137    1.291    1.824    1.000   normal(1, 3)
    x3                1.265    0.141    1.003    1.552    0.999   normal(1, 3)
  dem60 =~                                                                    
    y1                2.266    0.260    1.798    2.810    0.999   normal(1, 3)
    y2                3.060    0.398    2.298    3.871    1.000   normal(1, 3)
    y3                2.379    0.353    1.713    3.079    1.000   normal(1, 3)
    y4                2.964    0.317    2.384    3.639    1.000   normal(1, 3)
  dem65 =~                                                                    
    y5                2.133    0.263    1.663    2.696    1.000   normal(1, 3)
    y6                2.715    0.342    2.082    3.385    1.002   normal(1, 3)
    y7                2.754    0.325    2.150    3.421    1.002   normal(1, 3)
    y8                2.836    0.312    2.280    3.488    1.001   normal(1, 3)

Covariances:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior      
  ind60 ~~                                                                    
    dem60             0.426    0.105    0.209    0.616    1.002    lkj_corr(1)
    dem65             0.531    0.094    0.335    0.695    1.001    lkj_corr(1)
  dem60 ~~                                                                    
    dem65             0.952    0.030    0.879    0.993    1.002    lkj_corr(1)

Intercepts:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior      
   .x1                5.011    0.088    4.848    5.184    1.000    normal(3,2)
   .x2                4.697    0.178    4.361    5.045    1.001    normal(3,2)
   .x3                3.479    0.167    3.161    3.821    1.001    normal(3,2)
   .y1                5.191    0.293    4.610    5.764    1.001    normal(3,2)
   .y2                3.888    0.431    3.056    4.731    1.000    normal(3,2)
   .y3                6.230    0.377    5.493    6.979    1.000    normal(3,2)
   .y4                4.102    0.375    3.330    4.811    1.001    normal(3,2)
   .y5                4.872    0.290    4.271    5.419    1.001    normal(3,2)
   .y6                2.666    0.370    1.916    3.399    1.001    normal(3,2)
   .y7                5.843    0.364    5.143    6.546    1.001    normal(3,2)
   .y8                3.708    0.360    3.011    4.419    1.001    normal(3,2)
    ind60             0.000                                                   
    dem60             0.000                                                   
    dem65             0.000                                                   

Variances:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior      
   .x1                0.091    0.022    0.051    0.137    1.000 gamma(1,1)[sd]
   .x2                0.112    0.077    0.001    0.283    1.001 gamma(1,1)[sd]
   .x3                0.513    0.103    0.341    0.736    0.999 gamma(1,1)[sd]
   .y1                2.072    0.461    1.306    3.083    1.000 gamma(1,1)[sd]
   .y2                6.683    1.267    4.607    9.485    1.000 gamma(1,1)[sd]
   .y3                5.514    1.014    3.835    7.789    1.001 gamma(1,1)[sd]
   .y4                2.915    0.686    1.805    4.420    1.000 gamma(1,1)[sd]
   .y5                2.560    0.503    1.743    3.671    1.000 gamma(1,1)[sd]
   .y6                4.430    0.840    3.003    6.271    1.000 gamma(1,1)[sd]
   .y7                3.646    0.746    2.416    5.330    0.999 gamma(1,1)[sd]
   .y8                2.956    0.646    1.871    4.453    1.000 gamma(1,1)[sd]
    ind60             1.000                                                   
    dem60             1.000                                                   
    dem65             1.000                                                   

R-Square:
                   Estimate
    x1                0.842
    x2                0.955
    x3                0.757
    y1                0.713
    y2                0.584
    y3                0.507
    y4                0.751
    y5                0.640
    y6                0.625
    y7                0.675
    y8                0.731</code></pre>
</div>
</div>
</section>
<section id="cross-time-residuals" class="slide level2">
<h2>Cross time residuals</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-31_e0144e4f2e1af6c15ce1c6feb988f0a4">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a href="#cb29-1"></a>mod3 <span class="ot">&lt;-</span> <span class="st">'</span></span>
<span id="cb29-2"><a href="#cb29-2"></a><span class="st">  # latent variable definitions</span></span>
<span id="cb29-3"><a href="#cb29-3"></a><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span id="cb29-4"><a href="#cb29-4"></a><span class="st">     dem60 =~ y1 + y2 + y3 + y4</span></span>
<span id="cb29-5"><a href="#cb29-5"></a><span class="st">     dem65 =~ y5 + y6 + y7 + y8</span></span>
<span id="cb29-6"><a href="#cb29-6"></a></span>
<span id="cb29-7"><a href="#cb29-7"></a><span class="st">  # residual correlations</span></span>
<span id="cb29-8"><a href="#cb29-8"></a><span class="st">    y1 ~~ y5</span></span>
<span id="cb29-9"><a href="#cb29-9"></a><span class="st">    y2 ~~ y6</span></span>
<span id="cb29-10"><a href="#cb29-10"></a><span class="st">    y3 ~~ y7</span></span>
<span id="cb29-11"><a href="#cb29-11"></a><span class="st">    y4 ~~ y8</span></span>
<span id="cb29-12"><a href="#cb29-12"></a><span class="st">'</span></span>
<span id="cb29-13"><a href="#cb29-13"></a></span>
<span id="cb29-14"><a href="#cb29-14"></a>f3 <span class="ot">&lt;-</span> <span class="fu">bcfa</span>(mod3, <span class="at">data=</span>PoliticalDemocracy, </span>
<span id="cb29-15"><a href="#cb29-15"></a>           <span class="at">meanstructure=</span>T, <span class="at">std.lv=</span>T, <span class="at">dp=</span>priors,</span>
<span id="cb29-16"><a href="#cb29-16"></a>           <span class="at">burnin=</span><span class="dv">1000</span>, <span class="at">sample=</span><span class="dv">1000</span>, <span class="at">n.chains=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000207 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.07 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 4.635 seconds (Warm-up)
Chain 1:                4.184 seconds (Sampling)
Chain 1:                8.819 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000226 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.26 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 5.383 seconds (Warm-up)
Chain 2:                4.361 seconds (Sampling)
Chain 2:                9.744 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000298 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.98 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 4.908 seconds (Warm-up)
Chain 3:                4.705 seconds (Sampling)
Chain 3:                9.613 seconds (Total)
Chain 3: 
Computing post-estimation metrics (including lvs if requested)...</code></pre>
</div>
</div>
</section>
<section id="convergence-and-efficiency-2" class="slide level2">
<h2>Convergence and efficiency</h2>
<ul>
<li>Convergence</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-32_f126efe9a3444055a4a381b4f60aca7b">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a href="#cb31-1"></a><span class="fu">max</span>(<span class="fu">blavInspect</span>(f3, <span class="st">"psrf"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.002261</code></pre>
</div>
</div>
<ul>
<li>Efficiency</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-33_a172eb19675cd64ab66196b55154b2f5">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1"></a><span class="fu">min</span>(<span class="fu">blavInspect</span>(f3, <span class="st">"neff"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1056.703</code></pre>
</div>
</div>
</section>
<section id="parameter-posteriors-2" class="slide level2">
<h2>Parameter posteriors</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-34_eb4e7abb26eb376df27116406c8e8adf">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1"></a><span class="fu">summary</span>(f3, <span class="at">standardized=</span>T, <span class="at">rsquare=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>blavaan 0.5.3 ended normally after 1000 iterations

  Estimator                                      BAYES
  Optimization method                             MCMC
  Number of model parameters                        40

  Number of observations                            75

  Statistic                                 MargLogLik         PPP
  Value                                             NA       0.243

Parameter Estimates:


Latent Variables:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
  ind60 =~                                                              
    x1                0.698    0.070    0.571    0.851    0.698    0.917
    x2                1.536    0.139    1.286    1.829    1.536    0.978
    x3                1.269    0.139    1.013    1.557    1.269    0.872
  dem60 =~                                                              
    y1                2.211    0.281    1.674    2.786    2.211    0.823
    y2                3.054    0.417    2.298    3.905    3.054    0.761
    y3                2.331    0.361    1.669    3.087    2.331    0.698
    y4                3.055    0.343    2.424    3.767    3.055    0.884
  dem65 =~                                                              
    y5                2.072    0.279    1.561    2.663    2.072    0.775
    y6                2.727    0.358    2.068    3.465    2.727    0.791
    y7                2.764    0.336    2.168    3.466    2.764    0.819
    y8                2.910    0.329    2.329    3.594    2.910    0.873
     Rhat    Prior      
                        
    1.000   normal(1, 3)
    1.001   normal(1, 3)
    1.000   normal(1, 3)
                        
    1.001   normal(1, 3)
    1.000   normal(1, 3)
    1.000   normal(1, 3)
    1.001   normal(1, 3)
                        
    1.000   normal(1, 3)
    1.000   normal(1, 3)
    1.000   normal(1, 3)
    1.001   normal(1, 3)

Covariances:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
 .y1 ~~                                                                 
   .y5                0.914    0.414    0.185    1.812    0.914    0.354
 .y2 ~~                                                                 
   .y6                1.909    0.788    0.506    3.559    1.909    0.348
 .y3 ~~                                                                 
   .y7                1.300    0.670    0.083    2.716    1.300    0.280
 .y4 ~~                                                                 
   .y8                0.189    0.519   -0.761    1.255    0.189    0.072
  ind60 ~~                                                              
    dem60             0.425    0.106    0.209    0.616    0.425    0.425
    dem65             0.533    0.093    0.331    0.695    0.533    0.533
  dem60 ~~                                                              
    dem65             0.927    0.032    0.852    0.978    0.927    0.927
     Rhat    Prior      
                        
    1.000      beta(1,1)
                        
    1.000      beta(1,1)
                        
    0.999      beta(1,1)
                        
    1.000      beta(1,1)
                        
    1.001    lkj_corr(1)
    1.002    lkj_corr(1)
                        
    1.000    lkj_corr(1)

Intercepts:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
   .x1                5.010    0.087    4.838    5.179    5.010    6.584
   .x2                4.695    0.181    4.353    5.055    4.695    2.989
   .x3                3.479    0.167    3.156    3.806    3.479    2.390
   .y1                5.207    0.293    4.629    5.771    5.207    1.938
   .y2                3.914    0.436    3.045    4.747    3.914    0.976
   .y3                6.245    0.359    5.535    6.932    6.245    1.869
   .y4                4.127    0.371    3.404    4.863    4.127    1.194
   .y5                4.888    0.293    4.312    5.466    4.888    1.829
   .y6                2.688    0.376    1.961    3.419    2.688    0.779
   .y7                5.850    0.370    5.120    6.577    5.850    1.733
   .y8                3.726    0.358    3.047    4.422    3.726    1.117
    ind60             0.000                               0.000    0.000
    dem60             0.000                               0.000    0.000
    dem65             0.000                               0.000    0.000
     Rhat    Prior      
    1.002    normal(3,2)
    1.002    normal(3,2)
    1.002    normal(3,2)
    1.000    normal(3,2)
    1.001    normal(3,2)
    1.000    normal(3,2)
    1.001    normal(3,2)
    1.000    normal(3,2)
    1.002    normal(3,2)
    1.001    normal(3,2)
    1.001    normal(3,2)
                        
                        
                        

Variances:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
   .x1                0.092    0.022    0.052    0.139    0.092    0.159
   .x2                0.109    0.077    0.001    0.279    0.109    0.044
   .x3                0.509    0.099    0.346    0.730    0.509    0.240
   .y1                2.335    0.541    1.391    3.530    2.335    0.323
   .y2                6.756    1.302    4.574    9.628    6.756    0.420
   .y3                5.731    1.060    3.961    8.067    5.731    0.513
   .y4                2.605    0.788    1.217    4.321    2.605    0.218
   .y5                2.850    0.598    1.835    4.178    2.850    0.399
   .y6                4.463    0.909    2.979    6.576    4.463    0.375
   .y7                3.752    0.781    2.437    5.496    3.752    0.329
   .y8                2.653    0.702    1.399    4.159    2.653    0.239
    ind60             1.000                               1.000    1.000
    dem60             1.000                               1.000    1.000
    dem65             1.000                               1.000    1.000
     Rhat    Prior      
    1.001 gamma(1,1)[sd]
    1.001 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.001 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    0.999 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
                        
                        
                        

R-Square:
                   Estimate
    x1                0.841
    x2                0.956
    x3                0.760
    y1                0.677
    y2                0.580
    y3                0.487
    y4                0.782
    y5                0.601
    y6                0.625
    y7                0.671
    y8                0.761</code></pre>
</div>
</div>
</section>
<section id="cross-time-factor-loadings" class="slide level2">
<h2>Cross time factor loadings</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-35_03ef6adc138523bf6763e6aa0e8b9eee">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1"></a>mod4 <span class="ot">&lt;-</span> <span class="st">'</span></span>
<span id="cb37-2"><a href="#cb37-2"></a><span class="st">  # latent variable definitions</span></span>
<span id="cb37-3"><a href="#cb37-3"></a><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span id="cb37-4"><a href="#cb37-4"></a><span class="st">     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4</span></span>
<span id="cb37-5"><a href="#cb37-5"></a><span class="st">     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8</span></span>
<span id="cb37-6"><a href="#cb37-6"></a></span>
<span id="cb37-7"><a href="#cb37-7"></a><span class="st">  # residual correlations</span></span>
<span id="cb37-8"><a href="#cb37-8"></a><span class="st">    y1 ~~ y5</span></span>
<span id="cb37-9"><a href="#cb37-9"></a><span class="st">    y2 ~~ y6</span></span>
<span id="cb37-10"><a href="#cb37-10"></a><span class="st">    y3 ~~ y7</span></span>
<span id="cb37-11"><a href="#cb37-11"></a><span class="st">    y4 ~~ y8</span></span>
<span id="cb37-12"><a href="#cb37-12"></a><span class="st">'</span></span>
<span id="cb37-13"><a href="#cb37-13"></a></span>
<span id="cb37-14"><a href="#cb37-14"></a>f4 <span class="ot">&lt;-</span> <span class="fu">bcfa</span>(mod4, <span class="at">data=</span>PoliticalDemocracy, </span>
<span id="cb37-15"><a href="#cb37-15"></a>           <span class="at">meanstructure=</span>T, <span class="at">std.lv=</span>T, <span class="at">dp=</span>priors,</span>
<span id="cb37-16"><a href="#cb37-16"></a>           <span class="at">burnin=</span><span class="dv">1000</span>, <span class="at">sample=</span><span class="dv">1000</span>, <span class="at">n.chains=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000232 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.32 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 4.498 seconds (Warm-up)
Chain 1:                4.193 seconds (Sampling)
Chain 1:                8.691 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000215 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.15 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 5.401 seconds (Warm-up)
Chain 2:                4.127 seconds (Sampling)
Chain 2:                9.528 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000235 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.35 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 4.529 seconds (Warm-up)
Chain 3:                4.198 seconds (Sampling)
Chain 3:                8.727 seconds (Total)
Chain 3: 
Computing post-estimation metrics (including lvs if requested)...</code></pre>
</div>
</div>
</section>
<section id="convergence-and-efficiency-3" class="slide level2">
<h2>Convergence and efficiency</h2>
<ul>
<li>Convergence</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-36_2d51b75d6792a878830af0beed4dd890">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1"></a><span class="fu">max</span>(<span class="fu">blavInspect</span>(f4, <span class="st">"psrf"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.00439</code></pre>
</div>
</div>
<ul>
<li>Efficiency</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-37_174dd469a3694318db83f0f2b00e95e1">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1"></a><span class="fu">min</span>(<span class="fu">blavInspect</span>(f4, <span class="st">"neff"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1108.879</code></pre>
</div>
</div>
</section>
<section id="parameter-posteriors-3" class="slide level2">
<h2>Parameter posteriors</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-38_830fe251d7295d6dc46e6ad126c392b1">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a href="#cb43-1"></a><span class="fu">summary</span>(f4, <span class="at">standardized=</span>T, <span class="at">rsquare=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>blavaan 0.5.3 ended normally after 1000 iterations

  Estimator                                      BAYES
  Optimization method                             MCMC
  Number of model parameters                        40
  Number of equality constraints                     4

  Number of observations                            75

  Statistic                                 MargLogLik         PPP
  Value                                             NA       0.271

Parameter Estimates:


Latent Variables:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
  ind60 =~                                                              
    x1                0.700    0.070    0.574    0.850    0.700    0.918
    x2                1.536    0.140    1.281    1.840    1.536    0.977
    x3                1.269    0.140    1.015    1.554    1.269    0.871
  dem60 =~                                                              
    y1         (a)    2.135    0.245    1.691    2.659    2.135    0.816
    y2         (b)    2.815    0.329    2.212    3.519    2.815    0.730
    y3         (c)    2.585    0.304    2.042    3.246    2.585    0.737
    y4         (d)    2.952    0.286    2.430    3.549    2.952    0.875
  dem65 =~                                                              
    y5         (a)    2.135    0.245    1.691    2.659    2.135    0.787
    y6         (b)    2.815    0.329    2.212    3.519    2.815    0.802
    y7         (c)    2.585    0.304    2.042    3.246    2.585    0.795
    y8         (d)    2.952    0.286    2.430    3.549    2.952    0.878
     Rhat    Prior      
                        
    1.000   normal(1, 3)
    1.000   normal(1, 3)
    1.000   normal(1, 3)
                        
    1.000   normal(1, 3)
    1.000   normal(1, 3)
    1.000   normal(1, 3)
    1.000   normal(1, 3)
                        
    1.000               
    1.000               
    1.000               
    1.000               

Covariances:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
 .y1 ~~                                                                 
   .y5                0.886    0.414    0.157    1.780    0.886    0.350
 .y2 ~~                                                                 
   .y6                1.996    0.807    0.586    3.781    1.996    0.361
 .y3 ~~                                                                 
   .y7                1.233    0.665    0.037    2.700    1.233    0.264
 .y4 ~~                                                                 
   .y8                0.202    0.533   -0.776    1.311    0.202    0.077
  ind60 ~~                                                              
    dem60             0.435    0.101    0.230    0.619    0.435    0.435
    dem65             0.541    0.092    0.337    0.699    0.541    0.541
  dem60 ~~                                                              
    dem65             0.927    0.032    0.854    0.978    0.927    0.927
     Rhat    Prior      
                        
    1.000      beta(1,1)
                        
    1.000      beta(1,1)
                        
    1.002      beta(1,1)
                        
    0.999      beta(1,1)
                        
    1.000    lkj_corr(1)
    1.000    lkj_corr(1)
                        
    1.000    lkj_corr(1)

Intercepts:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
   .x1                5.007    0.086    4.833    5.179    5.007    6.567
   .x2                4.687    0.179    4.318    5.030    4.687    2.981
   .x3                3.467    0.167    3.137    3.800    3.467    2.379
   .y1                5.203    0.290    4.615    5.757    5.203    1.988
   .y2                3.927    0.422    3.093    4.764    3.927    1.018
   .y3                6.199    0.390    5.411    6.942    6.199    1.768
   .y4                4.106    0.373    3.363    4.819    4.106    1.218
   .y5                4.869    0.299    4.272    5.459    4.869    1.794
   .y6                2.660    0.378    1.908    3.384    2.660    0.758
   .y7                5.852    0.355    5.156    6.500    5.852    1.801
   .y8                3.698    0.360    2.978    4.377    3.698    1.100
    ind60             0.000                               0.000    0.000
    dem60             0.000                               0.000    0.000
    dem65             0.000                               0.000    0.000
     Rhat    Prior      
    1.000    normal(3,2)
    1.000    normal(3,2)
    1.000    normal(3,2)
    1.002    normal(3,2)
    1.003    normal(3,2)
    1.002    normal(3,2)
    1.002    normal(3,2)
    1.002    normal(3,2)
    1.004    normal(3,2)
    1.004    normal(3,2)
    1.003    normal(3,2)
                        
                        
                        

Variances:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
   .x1                0.092    0.022    0.053    0.139    0.092    0.157
   .x2                0.114    0.076    0.002    0.284    0.114    0.046
   .x3                0.514    0.102    0.347    0.748    0.514    0.242
   .y1                2.288    0.537    1.383    3.508    2.288    0.334
   .y2                6.953    1.305    4.772    9.872    6.953    0.467
   .y3                5.604    1.104    3.843    8.115    5.604    0.456
   .y4                2.660    0.777    1.315    4.324    2.660    0.234
   .y5                2.808    0.581    1.870    4.123    2.808    0.381
   .y6                4.403    0.860    2.911    6.312    4.403    0.357
   .y7                3.880    0.788    2.562    5.647    3.880    0.367
   .y8                2.594    0.679    1.436    4.075    2.594    0.229
    ind60             1.000                               1.000    1.000
    dem60             1.000                               1.000    1.000
    dem65             1.000                               1.000    1.000
     Rhat    Prior      
    1.000 gamma(1,1)[sd]
    1.002 gamma(1,1)[sd]
    0.999 gamma(1,1)[sd]
    0.999 gamma(1,1)[sd]
    0.999 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    0.999 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
                        
                        
                        

R-Square:
                   Estimate
    x1                0.843
    x2                0.954
    x3                0.758
    y1                0.666
    y2                0.533
    y3                0.544
    y4                0.766
    y5                0.619
    y6                0.643
    y7                0.633
    y8                0.771</code></pre>
</div>
</div>
</section></section>
<section>
<section id="model-fit-evaluation" class="title-slide slide level1 center">
<h1>Model fit evaluation</h1>

</section>
<section id="model-fit" class="slide level2">
<h2>Model fit</h2>
<p>“With respect to model fit, researchers do not seem adequately sensitive to the fundamental reality that there is no true model…, that all models are wrong to some degree, even in the population, and that the best one can hope for is to identify a parsimonious, substantively meaningful model that fits observed data adequately well. At the same time, one must recognize that there may well be other models that fit the data to approximately the same degree… It is clear that a finding of good fit does not imply that a model is correct or true, but only plausible” - MacCallum &amp; Austin, 2000</p>
</section>
<section id="posterior-predictive-p-value" class="slide level2 smaller">
<h2>Posterior predictive <span class="math inline">\(p\)</span>-value</h2>
<ul>
<li>Measure of the model’s absolute fit</li>
<li>Compares observed likelihood ratio test statistics to likelihood ratio test statistics generated from the model’s posterior predictive distribution.
<ul>
<li>Compute the observed LRT statistic</li>
<li>Generate artificial data from the model</li>
<li>Compute the posterior predictive LRT of the artificial data</li>
<li>Record which LRT is higher</li>
</ul></li>
<li>The <span class="math inline">\(PPP\)</span> is the proportion of times the posterior LRT is larger.</li>
<li>Perfect fit is 0.5</li>
</ul>
</section>
<section id="posterior-predictive-p-value-1" class="slide level2">
<h2>Posterior predictive <span class="math inline">\(p\)</span>-value</h2>
<ul>
<li>In practice behaves similar to the <span class="math inline">\(\chi^2\)</span> <span class="math inline">\(p\)</span>-value in frequentist SEM</li>
<li>As sample size increases will reject models for small deviations</li>
<li>Not recommended to use as general practice</li>
</ul>
</section>
<section id="overall-model-fit-approximate-indices" class="slide level2">
<h2>Overall model fit (approximate indices)</h2>
<ul>
<li>One of the first steps is to evaluate the model’s global fit</li>
<li>Commonly done by presenting multiple fit indices, with some of the most common being based on the model’s <span class="math inline">\(\chi^2\)</span>.</li>
<li>We have developed Bayesian versions of these indices <span class="citation" data-cites="garnier_adapting_2020">(<a href="#/references" role="doc-biblioref" onclick="">Garnier-Villarreal and Jorgensen 2020</a>)</span> that can be computed with <code>blavaan</code></li>
</ul>
</section>
<section id="noncentrality-based-fit-indices" class="slide level2">
<h2>Noncentrality-Based Fit Indices</h2>
<ul>
<li>This group of indices compares the hypothesized model against the perfect saturated model.</li>
<li>It specifically uses the noncentrality parameter <span class="math inline">\(\hat{\lambda} = \chi^2 - df\)</span>, with the <span class="math inline">\(df\)</span> being adjusted by different model/data characteristics.</li>
<li>Indices include Root Mean Square Error of approximation (RMSEA), McDonald’s centrality index (Mc), gamma-hat (<span class="math inline">\(\hat{\Gamma}\)</span>), and adjusted gamma-hat (<span class="math inline">\(\hat{\Gamma}_{adj}\)</span>).</li>
</ul>
</section>
<section id="incremental-fit-indices" class="slide level2">
<h2>Incremental Fit Indices</h2>
<ul>
<li>Compares the hypothesized model with the <em>worst</em> possible model, so they are called incremental indices.</li>
<li>Comparing your model’s <span class="math inline">\(\chi^2_H\)</span> to the <em>null</em> model’s <span class="math inline">\(\chi^2_0\)</span> in different ways.</li>
<li>Including the Comparative Fit Index (CFI), Tucker-Lewis Index (TLI), and Normed Fit Index (NFI).</li>
</ul>
</section>
<section id="model-fit-in-blavaan" class="slide level2">
<h2>Model fit in <code>blavaan</code></h2>
<ul>
<li>We can directly calculate the noncentrality fit indices</li>
<li>To estimate the incremental indices we need to estimate the <em>null</em> model for comparison</li>
</ul>
</section>
<section id="null-model" class="slide level2">
<h2>Null model</h2>
<ul>
<li>Standard null model: only estimate variances and means of the indicators</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-39_e203f34317f794af1f2111c3b08f9dda">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a href="#cb45-1"></a>HS.model_null <span class="ot">&lt;-</span> <span class="st">'</span></span>
<span id="cb45-2"><a href="#cb45-2"></a><span class="st">x1 ~~ x1 </span></span>
<span id="cb45-3"><a href="#cb45-3"></a><span class="st">x2 ~~ x2 </span></span>
<span id="cb45-4"><a href="#cb45-4"></a><span class="st">x3 ~~ x3</span></span>
<span id="cb45-5"><a href="#cb45-5"></a><span class="st">y1 ~~ y1</span></span>
<span id="cb45-6"><a href="#cb45-6"></a><span class="st">y2 ~~ y2</span></span>
<span id="cb45-7"><a href="#cb45-7"></a><span class="st">y3 ~~ y3</span></span>
<span id="cb45-8"><a href="#cb45-8"></a><span class="st">y4 ~~ y4</span></span>
<span id="cb45-9"><a href="#cb45-9"></a><span class="st">y5 ~~ y5</span></span>
<span id="cb45-10"><a href="#cb45-10"></a><span class="st">y6 ~~ y6</span></span>
<span id="cb45-11"><a href="#cb45-11"></a><span class="st">y7 ~~ y7</span></span>
<span id="cb45-12"><a href="#cb45-12"></a><span class="st">y8 ~~ y8'</span></span>
<span id="cb45-13"><a href="#cb45-13"></a></span>
<span id="cb45-14"><a href="#cb45-14"></a>fit_null <span class="ot">&lt;-</span> <span class="fu">bcfa</span>(HS.model_null, </span>
<span id="cb45-15"><a href="#cb45-15"></a>                 <span class="at">data=</span>PoliticalDemocracy,</span>
<span id="cb45-16"><a href="#cb45-16"></a>                 <span class="at">meanstructure=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000168 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.68 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 1500 [  0%]  (Warmup)
Chain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)
Chain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)
Chain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)
Chain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)
Chain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)
Chain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)
Chain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)
Chain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)
Chain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)
Chain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)
Chain 1: Iteration: 1500 / 1500 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 1.139 seconds (Warm-up)
Chain 1:                1.526 seconds (Sampling)
Chain 1:                2.665 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000173 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.73 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 1500 [  0%]  (Warmup)
Chain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)
Chain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)
Chain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)
Chain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)
Chain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)
Chain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)
Chain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)
Chain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)
Chain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)
Chain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)
Chain 2: Iteration: 1500 / 1500 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 1.127 seconds (Warm-up)
Chain 2:                1.565 seconds (Sampling)
Chain 2:                2.692 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000206 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.06 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 1500 [  0%]  (Warmup)
Chain 3: Iteration:  150 / 1500 [ 10%]  (Warmup)
Chain 3: Iteration:  300 / 1500 [ 20%]  (Warmup)
Chain 3: Iteration:  450 / 1500 [ 30%]  (Warmup)
Chain 3: Iteration:  501 / 1500 [ 33%]  (Sampling)
Chain 3: Iteration:  650 / 1500 [ 43%]  (Sampling)
Chain 3: Iteration:  800 / 1500 [ 53%]  (Sampling)
Chain 3: Iteration:  950 / 1500 [ 63%]  (Sampling)
Chain 3: Iteration: 1100 / 1500 [ 73%]  (Sampling)
Chain 3: Iteration: 1250 / 1500 [ 83%]  (Sampling)
Chain 3: Iteration: 1400 / 1500 [ 93%]  (Sampling)
Chain 3: Iteration: 1500 / 1500 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 1.195 seconds (Warm-up)
Chain 3:                1.603 seconds (Sampling)
Chain 3:                2.798 seconds (Total)
Chain 3: 
Computing post-estimation metrics (including lvs if requested)...</code></pre>
</div>
</div>
</section>
<section id="convergence-and-efficiency-4" class="slide level2">
<h2>Convergence and efficiency</h2>
<ul>
<li>Convergence</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-40_9f97942906e0a76e46b2666946d9ee5f">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a href="#cb47-1"></a><span class="fu">max</span>(<span class="fu">blavInspect</span>(fit_null, <span class="st">"psrf"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.000468</code></pre>
</div>
</div>
<ul>
<li>Efficiency</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-41_4571e92c74f7cb91891af2dcb661d5e1">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a href="#cb49-1"></a><span class="fu">min</span>(<span class="fu">blavInspect</span>(fit_null, <span class="st">"neff"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4100.233</code></pre>
</div>
</div>
</section>
<section id="bayesian-fit-indices" class="slide level2">
<h2>Bayesian fit indices</h2>
<ul>
<li>Basic measurement model (no loadings constraints or residual correlations)</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-42_4d4953640f492c1f5a8d813c854bcf5b">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a href="#cb51-1"></a>fits_all <span class="ot">&lt;-</span> <span class="fu">blavFitIndices</span>(f1, <span class="at">baseline.model =</span> fit_null)</span>
<span id="cb51-2"><a href="#cb51-2"></a><span class="fu">summary</span>(fits_all, <span class="at">central.tendency =</span> <span class="fu">c</span>(<span class="st">"mean"</span>,<span class="st">"median"</span>), <span class="at">prob =</span> .<span class="dv">90</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Posterior summary statistics and highest posterior density (HPD) 90% credible intervals for devm-based fit indices:

               EAP Median    SD lower upper
BRMSEA       0.101  0.100 0.013 0.078 0.122
BGammaHat    0.925  0.926 0.018 0.895 0.955
adjBGammaHat 0.867  0.870 0.032 0.814 0.920
BMc          0.801  0.804 0.047 0.725 0.878
BCFI         0.950  0.952 0.013 0.931 0.973
BTLI         0.932  0.934 0.018 0.905 0.963
BNFI         0.895  0.896 0.012 0.877 0.916</code></pre>
</div>
</div>
</section>
<section id="indices-posteriors-plots" class="slide level2">
<h2>Indices posteriors plots</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-43_2b4ae3d3197dc5052d89afefd89c78bb">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a href="#cb53-1"></a>dist_fits <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(fits_all<span class="sc">@</span>indices)</span>
<span id="cb53-2"><a href="#cb53-2"></a><span class="fu">mcmc_pairs</span>(dist_fits, <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">"BRMSEA"</span>,<span class="st">"BGammaHat"</span>,<span class="st">"BCFI"</span>),</span>
<span id="cb53-3"><a href="#cb53-3"></a>           <span class="at">diag_fun =</span> <span class="st">"hist"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-43-1.png" width="960" class="r-stretch"></section>
<section id="model-with-cross-time-parameters" class="slide level2">
<h2>Model with cross time parameters</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-44_3dd4423d3f77d17d1d4b6ce339c4c126">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb54-1"><a href="#cb54-1"></a>fits_all4 <span class="ot">&lt;-</span> <span class="fu">blavFitIndices</span>(f4, <span class="at">baseline.model =</span> fit_null)</span>
<span id="cb54-2"><a href="#cb54-2"></a><span class="fu">summary</span>(fits_all4, <span class="at">central.tendency =</span> <span class="fu">c</span>(<span class="st">"mean"</span>,<span class="st">"median"</span>), <span class="at">prob =</span> .<span class="dv">90</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Posterior summary statistics and highest posterior density (HPD) 90% credible intervals for devm-based fit indices:

               EAP Median    SD lower upper
BRMSEA       0.065  0.067 0.024 0.028 0.104
BGammaHat    0.966  0.968 0.020 0.940 1.000
adjBGammaHat 0.937  0.940 0.037 0.887 1.000
BMc          0.909  0.913 0.052 0.839 1.000
BCFI         0.978  0.980 0.013 0.961 1.000
BTLI         0.969  0.971 0.019 0.938 0.999
BNFI         0.924  0.926 0.012 0.904 0.943</code></pre>
</div>
</div>
</section>
<section id="model-with-cross-time-parameters-1" class="slide level2">
<h2>Model with cross time parameters</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-45_eda68fd7a58bd90d9549ea65bdaa4b19">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a href="#cb56-1"></a>dist_fits4 <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(fits_all4<span class="sc">@</span>indices)</span>
<span id="cb56-2"><a href="#cb56-2"></a><span class="fu">mcmc_pairs</span>(dist_fits4, <span class="at">pars =</span> <span class="fu">c</span>(<span class="st">"BRMSEA"</span>,<span class="st">"BGammaHat"</span>,<span class="st">"BCFI"</span>),</span>
<span id="cb56-3"><a href="#cb56-3"></a>           <span class="at">diag_fun =</span> <span class="st">"hist"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="BSEM_APS_2023_files/figure-revealjs/unnamed-chunk-45-1.png" width="960" class="r-stretch"></section>
<section id="bayesian-fit-indices-1" class="slide level2 smaller">
<h2>Bayesian fit indices</h2>
<ul>
<li>CFI and <span class="math inline">\(\hat{\Gamma}\)</span> are the most recommended indices <span class="citation" data-cites="garnier_adapting_2020">(<a href="#/references" role="doc-biblioref" onclick="">Garnier-Villarreal and Jorgensen 2020</a>)</span></li>
<li>They are less sensitive to data and model characteristics</li>
<li>Approximating “misfit” rather than other factors</li>
<li>Be careful with strict cutoffs to define “good” models
<ul>
<li>Closer to 1 means “better” fit</li>
</ul></li>
<li>They are effect size measures of misfit, rather than tests of it</li>
<li>Credible intervals allow us to evaluate uncertainty in model fit</li>
</ul>
</section></section>
<section>
<section id="model-comparison" class="title-slide slide level1 center">
<h1>Model comparison</h1>

</section>
<section id="ockams-razor" class="slide level2">
<h2>Ockam’s Razor</h2>
<p>Models with fewer assumptions are to be preferred</p>
</section>
<section id="statistical-errors" class="slide level2">
<h2>Statistical errors</h2>
<ul>
<li>Overfitting: leads to poor predictions by learning too much from the data.</li>
<li>Underfitting: leads to poor predictions by learning to little from data</li>
</ul>
</section>
<section id="information-and-uncertainty" class="slide level2">
<h2>Information and uncertainty</h2>
<ul>
<li>How much is our uncertainty reduce by learning an outcome?</li>
<li>Information: the reduction in uncertainty derived from learning an outcome.</li>
<li>Measure of uncertainty:
<ul>
<li>Continuous.</li>
<li>Increase as the number of events increase.</li>
<li>Should be additive.</li>
</ul></li>
</ul>
</section>
<section id="log-probability" class="slide level2">
<h2>Log-probability</h2>
<ul>
<li>The uncertainty contained in a probability distribution is the average log-probability of an event</li>
<li>Average log-probability of a model is the estimate of relative distance of the model from the target.</li>
<li>The bayesian log-probability score is Log- Pointwise-Predictive-Density (<span class="math inline">\(lppd\)</span>)</li>
<li><span class="math inline">\(lppd\)</span> estimates the deviance across all the posterior distribution (not wasting information)</li>
</ul>
</section>
<section id="what-prediction-do-we-care-about" class="slide level2">
<h2>What prediction do we care about?</h2>
<ul>
<li>What do we want our model to predict?</li>
<li>Predicting observed data is easy, and over estimates the model accuracy</li>
<li>Out of sample prediction tests the accuracy of predicting observations that are not included in the model. True test of model performance</li>
</ul>
</section>
<section id="what-prediction-do-we-care-about-1" class="slide level2">
<h2>What prediction do we care about?</h2>

<img data-src="out_sample.png" class="r-stretch"></section>
<section id="information-criteria" class="slide level2">
<h2>Information criteria</h2>
<ul>
<li>These methods intend to evaluate the out-of-sample predictive accuracy of the models, and compare that performance. This is the ability to predict a datapoint that hasn’t been used in the <strong>training</strong> model <span class="citation" data-cites="mcelreath_statistical_2020">(<a href="#/references" role="doc-biblioref" onclick="">McElreath 2020</a>)</span></li>
<li>Hard to interpret by themselves, good for comparison</li>
<li><span class="math inline">\(DIC\)</span>: Deviance Information criteria</li>
<li><span class="math inline">\(WAIC\)</span>: widely applicable information criteria</li>
<li><span class="math inline">\(LOO\)</span>: Leave-one-out information criteria</li>
</ul>
</section>
<section id="dic" class="slide level2">
<h2>DIC</h2>
<ul>
<li>Based on the overall model log-likelihood</li>
<li>Penalized by the effective number of parameters (<span class="math inline">\(efp\)</span>)</li>
<li>Lower values indicates better fit</li>
<li>Ignores the posterior distribution variability</li>
</ul>
<p><span class="math inline">\(DIC = -2LL + 2efp\)</span></p>
</section>
<section id="waic" class="slide level2">
<h2>WAIC</h2>
<ul>
<li>WAIC <span class="citation" data-cites="watanabeAsymptoticEquivalenceBayesa">(<a href="#/references" role="doc-biblioref" onclick="">Watanabe 2010</a>)</span> fully Bayesian generalization of the Akaike Information Criteria (AIC), where we have a measure of uncertainty/information of the model prediction for each row in the data across all posterior draws</li>
<li>This is the Log-Pointwise-Predictive-Density (lppd). The WAIC is defined as</li>
</ul>
<p><span class="math inline">\(WAIC= -2lppd + 2efp_{WAIC}\)</span></p>
</section>
<section id="loo" class="slide level2">
<h2>LOO</h2>
<ul>
<li>The LOO measures the predictive density of each observation holding out one observation at the time and use the rest of the observations to update the prior.</li>
<li>This estimation is calculated via <span class="citation" data-cites="vehtari_practical_2017">(<a href="#/references" role="doc-biblioref" onclick="">Vehtari, Gelman, and Gabry 2017</a>)</span>:</li>
</ul>
<p><span class="math inline">\(LOO = -2\sum_{i=1}^{n} log \Bigg(\frac{\sum^{S}_{s =1} w^{s}_{i}f(y_{i}|\theta^{s})}{\sum^{s}_{s=1} w^{s}_{i}}\Bigg)\)</span></p>
</section>
<section id="model-comparison-1" class="slide level2 smaller">
<h2>Model comparison</h2>
<ul>
<li>Both WAIC and LOO approximate the models’ performance across posterior draws, we are able to calculate a standard error for them and for model comparisons involving them.</li>
<li>Differences estimate the differences across the Expected Log-Pointwise-Predictive-Density (<span class="math inline">\(elpd\)</span>), and the standard error of the respective difference.</li>
<li>There are no clear cutoff rules on how to interpret and present these comparisons, and the researchers need to use their expert knowledge as part of the decision process.</li>
<li>The best recommendation is the present the differences in <span class="math inline">\(elpd\)</span> (<span class="math inline">\(\Delta elpd\)</span>), the standard error, and the ratio between them. If the ratio is at least 2 can be consider evidence of differences between the models, and a ratio of 4 would be considered stronger evidence.</li>
</ul>
</section>
<section id="bayes-factor" class="slide level2 smaller">
<h2>Bayes factor</h2>
<ul>
<li>In the Bayesian literature you will see the use of the Bayes factor (BF) to compare models.</li>
<li>There are a number of criticisms related to the use of the BF in BSEM, including (1) the BF is unstable for large models (like most SEMs), (2) it is highly sensitive to model priors, (3) it requires strong priors to have stable estimation of it, (4) it can require large number of posterior draws, (5) the estimation using the marginal likelihood ignores a lot of information from the posterior distributions.</li>
<li>For more details on this discussion please see <span class="citation" data-cites="tendeiro_review_2019">Tendeiro and Kiers (<a href="#/references" role="doc-biblioref" onclick="">2019</a>)</span> and <span class="citation" data-cites="schad_workflow_2022">Schad et al. (<a href="#/references" role="doc-biblioref" onclick="">2022</a>)</span>.</li>
<li>These criticisms lead us to recommend against use of the BF in everyday BSEM estimation.</li>
<li>For researchers who commit to their prior distributions and who commit to exploring the noise in their computations, the BF can used to describe the relative odds of one model over another, which is more intuitive than some other model comparison metrics.</li>
</ul>
</section>
<section id="model-comparison-2" class="slide level2 smaller">
<h2>Model comparison</h2>
<ul>
<li>Default priors vs weakly informative priors</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-46_a087ab80f76364feb4ef9bc791d93fb2">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a href="#cb57-1"></a>bc12 <span class="ot">&lt;-</span> <span class="fu">blavCompare</span>(f1, f2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
WAIC estimates: 
 object1:  3203.226 
 object2:  3202.516 

 ELPD difference &amp; SE: 
   -0.355    1.088 

LOO estimates: 
 object1:  3203.562 
 object2:  3202.974 

 ELPD difference &amp; SE: 
   -0.294    1.091 

Laplace approximation to the log-Bayes factor
(experimental; positive values favor object1):  -29.297 </code></pre>
</div>
</div>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-47_761355304bf553110f686e7f4edae492">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a href="#cb59-1"></a><span class="fu">abs</span>(bc12<span class="sc">$</span>diff_loo[,<span class="st">"elpd_diff"</span>] <span class="sc">/</span> bc12<span class="sc">$</span>diff_loo[,<span class="st">"se_diff"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   model2    model1 
      NaN 0.2695186 </code></pre>
</div>
</div>
</section>
<section id="model-comparison-3" class="slide level2">
<h2>Model comparison</h2>
<ul>
<li>Default priors vs weakly informative priors
<ul>
<li>Small difference indicate that both models have similar out-of-sample predictive accuracy</li>
<li>Can choose either model to continue (theory), I am keeping the weakly informative priors model</li>
</ul></li>
</ul>
</section>
<section id="model-comparison-4" class="slide level2 smaller">
<h2>Model comparison</h2>
<ul>
<li>Should we keep the residual correlations?</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-48_fa5bb23295b0b2a16790379274282498">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a href="#cb61-1"></a>bc23 <span class="ot">&lt;-</span> <span class="fu">blavCompare</span>(f2, f3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
WAIC estimates: 
 object1:  3202.516 
 object2:  3190.852 

 ELPD difference &amp; SE: 
   -5.832    5.943 

LOO estimates: 
 object1:  3202.974 
 object2:  3191.287 

 ELPD difference &amp; SE: 
   -5.844    5.949 

Laplace approximation to the log-Bayes factor
(experimental; positive values favor object1):       NA </code></pre>
</div>
</div>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-49_40d658a10801a9bdae7f0bfaa7fc01a2">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a href="#cb63-1"></a><span class="fu">abs</span>(bc23<span class="sc">$</span>diff_loo[,<span class="st">"elpd_diff"</span>] <span class="sc">/</span> bc23<span class="sc">$</span>diff_loo[,<span class="st">"se_diff"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   model2    model1 
      NaN 0.9823062 </code></pre>
</div>
</div>
</section>
<section id="model-comparison-5" class="slide level2">
<h2>Model comparison</h2>
<ul>
<li>Should we keep the residual correlations?
<ul>
<li>Small difference indicate that both models have similar out-of-sample predictive accuracy</li>
<li>Can choose either model to continue (theory), I am keeping the residual correlations model</li>
<li>Because the correlations are above <span class="math inline">\(r &gt; 0.2\)</span> and they are theoretically relevant</li>
</ul></li>
</ul>
</section>
<section id="model-comparison-6" class="slide level2 smaller">
<h2>Model comparison</h2>
<ul>
<li>Should we keep the equality constraints in factor loadings?</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-50_a7eca4b2d985f18f0c897b01a5d713d1">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a href="#cb65-1"></a>bc34 <span class="ot">&lt;-</span> <span class="fu">blavCompare</span>(f3, f4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
WAIC estimates: 
 object1:  3190.852 
 object2:  3186.936 

 ELPD difference &amp; SE: 
   -1.958    1.703 

LOO estimates: 
 object1:  3191.287 
 object2:  3187.39 

 ELPD difference &amp; SE: 
   -1.948    1.709 

Laplace approximation to the log-Bayes factor
(experimental; positive values favor object1):       NA </code></pre>
</div>
</div>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-51_167ce871c2073214899a3f0f7ae470f8">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a href="#cb67-1"></a><span class="fu">abs</span>(bc34<span class="sc">$</span>diff_loo[,<span class="st">"elpd_diff"</span>] <span class="sc">/</span> bc34<span class="sc">$</span>diff_loo[,<span class="st">"se_diff"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  model2   model1 
     NaN 1.139933 </code></pre>
</div>
</div>
</section>
<section id="model-comparison-7" class="slide level2">
<h2>Model comparison</h2>
<ul>
<li>Should we keep the equality constraints in factor loadings?
<ul>
<li>Small difference indicate that both models have similar out-of-sample predictive accuracy</li>
<li>Can choose either model to continue (theory), I am keeping the constrained mode</li>
<li>Because the constraints are theoretically relevant (longitudinal equivalence)</li>
</ul></li>
</ul>
</section>
<section id="model-comparison-8" class="slide level2 smaller">
<h2>Model comparison</h2>
<ul>
<li>Should we keep the factor correlations?</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-52_d669560b807a5e30ea329adb37f67153">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a href="#cb69-1"></a>mod5 <span class="ot">&lt;-</span> <span class="st">'</span></span>
<span id="cb69-2"><a href="#cb69-2"></a><span class="st">  # latent variable definitions</span></span>
<span id="cb69-3"><a href="#cb69-3"></a><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span id="cb69-4"><a href="#cb69-4"></a><span class="st">     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4</span></span>
<span id="cb69-5"><a href="#cb69-5"></a><span class="st">     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8</span></span>
<span id="cb69-6"><a href="#cb69-6"></a><span class="st">     </span></span>
<span id="cb69-7"><a href="#cb69-7"></a><span class="st">     ind60 ~~ 0*dem60 + 0*dem65</span></span>
<span id="cb69-8"><a href="#cb69-8"></a><span class="st">     dem60 ~~ 0*dem65</span></span>
<span id="cb69-9"><a href="#cb69-9"></a></span>
<span id="cb69-10"><a href="#cb69-10"></a><span class="st">  # residual correlations</span></span>
<span id="cb69-11"><a href="#cb69-11"></a><span class="st">    y1 ~~ y5</span></span>
<span id="cb69-12"><a href="#cb69-12"></a><span class="st">    y2 ~~ y6</span></span>
<span id="cb69-13"><a href="#cb69-13"></a><span class="st">    y3 ~~ y7</span></span>
<span id="cb69-14"><a href="#cb69-14"></a><span class="st">    y4 ~~ y8</span></span>
<span id="cb69-15"><a href="#cb69-15"></a><span class="st">'</span></span>
<span id="cb69-16"><a href="#cb69-16"></a></span>
<span id="cb69-17"><a href="#cb69-17"></a>f5 <span class="ot">&lt;-</span> <span class="fu">bcfa</span>(mod5, <span class="at">data=</span>PoliticalDemocracy, </span>
<span id="cb69-18"><a href="#cb69-18"></a>           <span class="at">meanstructure=</span>T, <span class="at">std.lv=</span>T, <span class="at">dp=</span>priors,</span>
<span id="cb69-19"><a href="#cb69-19"></a>           <span class="at">burnin=</span><span class="dv">1000</span>, <span class="at">sample=</span><span class="dv">1000</span>, <span class="at">n.chains=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000329 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 3.29 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 5.183 seconds (Warm-up)
Chain 1:                5.007 seconds (Sampling)
Chain 1:                10.19 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000218 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.18 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 4.983 seconds (Warm-up)
Chain 2:                4.126 seconds (Sampling)
Chain 2:                9.109 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000211 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.11 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 5.179 seconds (Warm-up)
Chain 3:                4.374 seconds (Sampling)
Chain 3:                9.553 seconds (Total)
Chain 3: 
Computing post-estimation metrics (including lvs if requested)...</code></pre>
</div>
</div>
</section>
<section id="model-comparison-9" class="slide level2 smaller">
<h2>Model comparison</h2>
<ul>
<li>Should we keep the factor correlations?</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-53_a4517b56b9deb093057a553b5c8de521">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a href="#cb71-1"></a>bc45 <span class="ot">&lt;-</span> <span class="fu">blavCompare</span>(f4, f5)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
WAIC estimates: 
 object1:  3186.936 
 object2:  3299.38 

 ELPD difference &amp; SE: 
  -56.222    9.293 

LOO estimates: 
 object1:  3187.39 
 object2:  3299.815 

 ELPD difference &amp; SE: 
  -56.213    9.310 

Laplace approximation to the log-Bayes factor
(experimental; positive values favor object1):       NA </code></pre>
</div>
</div>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-54_307048fadc77bdbd285e989c24bc58d9">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a href="#cb73-1"></a><span class="fu">abs</span>(bc45<span class="sc">$</span>diff_loo[,<span class="st">"elpd_diff"</span>] <span class="sc">/</span> bc45<span class="sc">$</span>diff_loo[,<span class="st">"se_diff"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  model1   model2 
     NaN 6.037976 </code></pre>
</div>
</div>
</section>
<section id="model-comparison-10" class="slide level2">
<h2>Model comparison</h2>
<ul>
<li>Should we keep the factor correlations?
<ul>
<li>Large difference indicates that there is difference between the models accuracy</li>
<li>Model with factor correlations fits better</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="bsem-latent-regression" class="title-slide slide level1 center">
<h1>BSEM: latent regression</h1>

</section>
<section id="latent-regressions" class="slide level2">
<h2>Latent Regressions</h2>
<ul>
<li>Switch from correlations to regressions</li>
<li>Theoretically meaningful relations</li>
<li>Add priors for the regression slopes</li>
</ul>
</section>
<section id="latent-regressions-1" class="slide level2">
<h2>Latent Regressions</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-55_739f7b642971214901530a6e49b5d817">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a href="#cb75-1"></a>priors <span class="ot">&lt;-</span> <span class="fu">dpriors</span>(<span class="at">nu=</span><span class="st">"normal(3,2)"</span>,</span>
<span id="cb75-2"><a href="#cb75-2"></a>                  <span class="at">lambda=</span><span class="st">"normal(1, 3)"</span>,</span>
<span id="cb75-3"><a href="#cb75-3"></a>                  <span class="at">beta=</span><span class="st">"normal(0.4, 2)"</span>,</span>
<span id="cb75-4"><a href="#cb75-4"></a>                  <span class="at">theta=</span><span class="st">"gamma(1,1)[sd]"</span>)</span>
<span id="cb75-5"><a href="#cb75-5"></a></span>
<span id="cb75-6"><a href="#cb75-6"></a>mod6 <span class="ot">&lt;-</span> <span class="st">'</span></span>
<span id="cb75-7"><a href="#cb75-7"></a><span class="st">  # latent variable definitions</span></span>
<span id="cb75-8"><a href="#cb75-8"></a><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span id="cb75-9"><a href="#cb75-9"></a><span class="st">     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4</span></span>
<span id="cb75-10"><a href="#cb75-10"></a><span class="st">     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8</span></span>
<span id="cb75-11"><a href="#cb75-11"></a></span>
<span id="cb75-12"><a href="#cb75-12"></a><span class="st">  # regressions</span></span>
<span id="cb75-13"><a href="#cb75-13"></a><span class="st">    dem60 ~ ind60</span></span>
<span id="cb75-14"><a href="#cb75-14"></a><span class="st">    dem65 ~ ind60 + dem60</span></span>
<span id="cb75-15"><a href="#cb75-15"></a></span>
<span id="cb75-16"><a href="#cb75-16"></a><span class="st">  # residual correlations</span></span>
<span id="cb75-17"><a href="#cb75-17"></a><span class="st">    y1 ~~ y5</span></span>
<span id="cb75-18"><a href="#cb75-18"></a><span class="st">    y2 ~~ y6</span></span>
<span id="cb75-19"><a href="#cb75-19"></a><span class="st">    y3 ~~ y7</span></span>
<span id="cb75-20"><a href="#cb75-20"></a><span class="st">    y4 ~~ y8</span></span>
<span id="cb75-21"><a href="#cb75-21"></a><span class="st">'</span></span>
<span id="cb75-22"><a href="#cb75-22"></a></span>
<span id="cb75-23"><a href="#cb75-23"></a>f6 <span class="ot">&lt;-</span> <span class="fu">bsem</span>(mod6, <span class="at">data=</span>PoliticalDemocracy, </span>
<span id="cb75-24"><a href="#cb75-24"></a>           <span class="at">meanstructure=</span>T, <span class="at">std.lv=</span>T, <span class="at">dp=</span>priors,</span>
<span id="cb75-25"><a href="#cb75-25"></a>           <span class="at">burnin=</span><span class="dv">1000</span>, <span class="at">sample=</span><span class="dv">1000</span>, <span class="at">n.chains=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000244 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.44 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 4.232 seconds (Warm-up)
Chain 1:                3.63 seconds (Sampling)
Chain 1:                7.862 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000223 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.23 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 4.406 seconds (Warm-up)
Chain 2:                4.487 seconds (Sampling)
Chain 2:                8.893 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000349 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 3.49 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 4.477 seconds (Warm-up)
Chain 3:                4.144 seconds (Sampling)
Chain 3:                8.621 seconds (Total)
Chain 3: 
Computing post-estimation metrics (including lvs if requested)...</code></pre>
</div>
</div>
</section>
<section id="convergence-and-efficiency-5" class="slide level2">
<h2>Convergence and efficiency</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-56_0d5a57d2e0a04927ff3dd3729a136a3b">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a href="#cb77-1"></a><span class="fu">max</span>(<span class="fu">blavInspect</span>(f6, <span class="st">"psrf"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1.002582</code></pre>
</div>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a href="#cb79-1"></a><span class="fu">min</span>(<span class="fu">blavInspect</span>(f6,<span class="st">"neff"</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1019.55</code></pre>
</div>
</div>
</section>
<section id="parameter-posteriors-4" class="slide level2">
<h2>Parameter posteriors</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-57_70f1029353312eb053d079352af49a6f">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a href="#cb81-1"></a><span class="fu">summary</span>(f6, <span class="at">standardize=</span>T, <span class="at">rsquare=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>blavaan 0.5.3 ended normally after 1000 iterations

  Estimator                                      BAYES
  Optimization method                             MCMC
  Number of model parameters                        40
  Number of equality constraints                     4

  Number of observations                            75

  Statistic                                 MargLogLik         PPP
  Value                                             NA       0.005

Parameter Estimates:


Latent Variables:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
  ind60 =~                                                              
    x1                0.706    0.073    0.577    0.866    0.706    0.922
    x2                1.544    0.146    1.284    1.863    1.544    0.974
    x3                1.280    0.143    1.023    1.588    1.280    0.873
  dem60 =~                                                              
    y1         (a)    1.410    0.168    1.090    1.748    1.725    0.733
    y2         (b)    1.913    0.223    1.495    2.370    2.340    0.656
    y3         (c)    1.769    0.200    1.392    2.183    2.163    0.677
    y4         (d)    2.103    0.191    1.743    2.493    2.572    0.855
  dem65 =~                                                              
    y5         (a)    1.410    0.168    1.090    1.748    2.168    0.776
    y6         (b)    1.913    0.223    1.495    2.370    2.941    0.822
    y7         (c)    1.769    0.200    1.392    2.183    2.719    0.805
    y8         (d)    2.103    0.191    1.743    2.493    3.233    0.913
     Rhat    Prior      
                        
    1.001   normal(1, 3)
    1.002   normal(1, 3)
    1.000   normal(1, 3)
                        
    1.000   normal(1, 3)
    1.000   normal(1, 3)
    0.999   normal(1, 3)
    1.001   normal(1, 3)
                        
    1.000               
    1.000               
    0.999               
    1.001               

Regressions:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
  dem60 ~                                                               
    ind60             0.704    0.166    0.386    1.052    0.576    0.576
  dem65 ~                                                               
    ind60             0.223    0.167   -0.111    0.563    0.145    0.145
    dem60             0.838    0.118    0.611    1.076    0.667    0.667
     Rhat    Prior      
                        
    1.000 normal(0.4, 2)
                        
    1.000 normal(0.4, 2)
    1.000 normal(0.4, 2)

Covariances:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
 .y1 ~~                                                                 
   .y5                0.872    0.441    0.107    1.860    0.872    0.308
 .y2 ~~                                                                 
   .y6                2.137    0.781    0.795    3.846    2.137    0.388
 .y3 ~~                                                                 
   .y7                1.502    0.685    0.319    2.946    1.502    0.319
 .y4 ~~                                                                 
   .y8                0.300    0.505   -0.625    1.348    0.300    0.133
     Rhat    Prior      
                        
    1.002      beta(1,1)
                        
    1.000      beta(1,1)
                        
    1.001      beta(1,1)
                        
    1.002      beta(1,1)

Intercepts:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
   .x1                5.004    0.086    4.831    5.171    5.004    6.534
   .x2                4.684    0.176    4.341    5.025    4.684    2.956
   .x3                3.466    0.165    3.135    3.779    3.466    2.364
   .y1                5.269    0.260    4.751    5.780    5.269    2.238
   .y2                3.997    0.396    3.193    4.755    3.997    1.120
   .y3                6.272    0.361    5.542    6.959    6.272    1.963
   .y4                4.186    0.332    3.524    4.811    4.186    1.392
   .y5                4.905    0.308    4.295    5.508    4.905    1.755
   .y6                2.692    0.383    1.944    3.422    2.692    0.752
   .y7                5.878    0.371    5.142    6.584    5.878    1.740
   .y8                3.726    0.383    2.978    4.486    3.726    1.052
    ind60             0.000                               0.000    0.000
   .dem60             0.000                               0.000    0.000
   .dem65             0.000                               0.000    0.000
     Rhat    Prior      
    1.002    normal(3,2)
    1.002    normal(3,2)
    1.003    normal(3,2)
    1.001    normal(3,2)
    1.001    normal(3,2)
    1.002    normal(3,2)
    1.002    normal(3,2)
    1.002    normal(3,2)
    1.001    normal(3,2)
    1.002    normal(3,2)
    1.002    normal(3,2)
                        
                        
                        

Variances:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
   .x1                0.088    0.022    0.050    0.136    0.088    0.150
   .x2                0.127    0.079    0.002    0.291    0.127    0.051
   .x3                0.510    0.103    0.337    0.734    0.510    0.237
   .y1                2.568    0.567    1.591    3.772    2.568    0.463
   .y2                7.268    1.308    5.067   10.124    7.268    0.570
   .y3                5.532    1.083    3.709    7.980    5.532    0.542
   .y4                2.427    0.792    1.055    4.111    2.427    0.268
   .y5                3.113    0.626    2.063    4.505    3.113    0.399
   .y6                4.166    0.853    2.738    6.072    4.166    0.325
   .y7                4.019    0.846    2.596    5.886    4.019    0.352
   .y8                2.101    0.695    0.898    3.627    2.101    0.167
    ind60             1.000                               1.000    1.000
   .dem60             1.000                               0.668    0.668
   .dem65             1.000                               0.423    0.423
     Rhat    Prior      
    1.000 gamma(1,1)[sd]
    1.002 gamma(1,1)[sd]
    0.999 gamma(1,1)[sd]
    1.001 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.001 gamma(1,1)[sd]
    1.001 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    0.999 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.001 gamma(1,1)[sd]
                        
                        
                        

R-Square:
                   Estimate
    x1                0.850
    x2                0.949
    x3                0.763
    y1                0.537
    y2                0.430
    y3                0.458
    y4                0.732
    y5                0.601
    y6                0.675
    y7                0.648
    y8                0.833
    dem60             0.332
    dem65             0.577</code></pre>
</div>
</div>
</section>
<section id="constrain-regressions-with-priors" class="slide level2">
<h2>Constrain regressions with priors</h2>
<ul>
<li>Can constrain parameters to be close to 0, but not exactly 0</li>
<li>Allows space the parameter to move out of the constraint if the data requires it</li>
<li>Can state specific hypothesis with priors</li>
</ul>
</section>
<section id="constrain-regressions-with-priors-1" class="slide level2">
<h2>Constrain regressions with priors</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-58_2066871000554c06e0f8667b34a429d1">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a href="#cb83-1"></a>mod7 <span class="ot">&lt;-</span> <span class="st">'</span></span>
<span id="cb83-2"><a href="#cb83-2"></a><span class="st">  # latent variable definitions</span></span>
<span id="cb83-3"><a href="#cb83-3"></a><span class="st">     ind60 =~ x1 + x2 + x3</span></span>
<span id="cb83-4"><a href="#cb83-4"></a><span class="st">     dem60 =~ a*y1 + b*y2 + c*y3 + d*y4</span></span>
<span id="cb83-5"><a href="#cb83-5"></a><span class="st">     dem65 =~ a*y5 + b*y6 + c*y7 + d*y8</span></span>
<span id="cb83-6"><a href="#cb83-6"></a></span>
<span id="cb83-7"><a href="#cb83-7"></a><span class="st">  # regressions</span></span>
<span id="cb83-8"><a href="#cb83-8"></a><span class="st">    dem60 ~ ind60</span></span>
<span id="cb83-9"><a href="#cb83-9"></a><span class="st">    dem65 ~ prior("normal(0,.08)")*ind60 + dem60</span></span>
<span id="cb83-10"><a href="#cb83-10"></a></span>
<span id="cb83-11"><a href="#cb83-11"></a><span class="st">  # residual correlations</span></span>
<span id="cb83-12"><a href="#cb83-12"></a><span class="st">    y1 ~~ y5</span></span>
<span id="cb83-13"><a href="#cb83-13"></a><span class="st">    y2 ~~ y6</span></span>
<span id="cb83-14"><a href="#cb83-14"></a><span class="st">    y3 ~~ y7</span></span>
<span id="cb83-15"><a href="#cb83-15"></a><span class="st">    y4 ~~ y8</span></span>
<span id="cb83-16"><a href="#cb83-16"></a><span class="st">'</span></span>
<span id="cb83-17"><a href="#cb83-17"></a></span>
<span id="cb83-18"><a href="#cb83-18"></a>f7 <span class="ot">&lt;-</span> <span class="fu">bsem</span>(mod7, <span class="at">data=</span>PoliticalDemocracy, </span>
<span id="cb83-19"><a href="#cb83-19"></a>           <span class="at">meanstructure=</span>T, <span class="at">std.lv=</span>T, <span class="at">dp=</span>priors,</span>
<span id="cb83-20"><a href="#cb83-20"></a>           <span class="at">burnin=</span><span class="dv">1000</span>, <span class="at">sample=</span><span class="dv">1000</span>, <span class="at">n.chains=</span><span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000331 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 3.31 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 6.576 seconds (Warm-up)
Chain 1:                3.897 seconds (Sampling)
Chain 1:                10.473 seconds (Total)
Chain 1: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000203 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 2.03 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 5.155 seconds (Warm-up)
Chain 2:                5.412 seconds (Sampling)
Chain 2:                10.567 seconds (Total)
Chain 2: 

SAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000218 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 2.18 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)
Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)
Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)
Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)
Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)
Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)
Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)
Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)
Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)
Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)
Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)
Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 4.646 seconds (Warm-up)
Chain 3:                3.644 seconds (Sampling)
Chain 3:                8.29 seconds (Total)
Chain 3: 
Computing post-estimation metrics (including lvs if requested)...</code></pre>
</div>
</div>
</section>
<section id="parameter-posteriors-5" class="slide level2">
<h2>Parameter posteriors</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-59_ffc5f8a80b15d15adc0b1a41a75df990">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a href="#cb85-1"></a><span class="fu">summary</span>(f7, <span class="at">standardize=</span>T, <span class="at">rsquare=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>blavaan 0.5.3 ended normally after 1000 iterations

  Estimator                                      BAYES
  Optimization method                             MCMC
  Number of model parameters                        40
  Number of equality constraints                     4

  Number of observations                            75

  Statistic                                 MargLogLik         PPP
  Value                                             NA       0.006

Parameter Estimates:


Latent Variables:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
  ind60 =~                                                              
    x1                0.698    0.074    0.567    0.856    0.698    0.920
    x2                1.527    0.145    1.269    1.847    1.527    0.973
    x3                1.266    0.143    1.015    1.580    1.266    0.871
  dem60 =~                                                              
    y1         (a)    1.403    0.165    1.084    1.737    1.722    0.728
    y2         (b)    1.922    0.218    1.517    2.356    2.359    0.658
    y3         (c)    1.767    0.199    1.396    2.187    2.169    0.676
    y4         (d)    2.114    0.194    1.744    2.498    2.594    0.860
  dem65 =~                                                              
    y5         (a)    1.403    0.165    1.084    1.737    2.116    0.765
    y6         (b)    1.922    0.218    1.517    2.356    2.899    0.818
    y7         (c)    1.767    0.199    1.396    2.187    2.665    0.798
    y8         (d)    2.114    0.194    1.744    2.498    3.187    0.912
     Rhat    Prior      
                        
    1.000   normal(1, 3)
    1.001   normal(1, 3)
    1.001   normal(1, 3)
                        
    0.999   normal(1, 3)
    1.004   normal(1, 3)
    1.000   normal(1, 3)
    1.006   normal(1, 3)
                        
    0.999               
    1.004               
    1.000               
    1.006               

Regressions:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
  dem60 ~                                                               
    ind60             0.711    0.171    0.404    1.071    0.580    0.580
  dem65 ~                                                               
    ind60             0.045    0.071   -0.091    0.186    0.030    0.030
    dem60             0.898    0.109    0.675    1.114    0.731    0.731
     Rhat    Prior      
                        
    1.000 normal(0.4, 2)
                        
    0.999  normal(0,.08)
    1.000 normal(0.4, 2)

Covariances:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
 .y1 ~~                                                                 
   .y5                0.887    0.466    0.063    1.903    0.887    0.307
 .y2 ~~                                                                 
   .y6                2.130    0.818    0.700    3.927    2.130    0.387
 .y3 ~~                                                                 
   .y7                1.521    0.688    0.308    3.014    1.521    0.320
 .y4 ~~                                                                 
   .y8                0.272    0.490   -0.605    1.323    0.272    0.123
     Rhat    Prior      
                        
    1.001      beta(1,1)
                        
    1.000      beta(1,1)
                        
    1.001      beta(1,1)
                        
    1.002      beta(1,1)

Intercepts:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
   .x1                5.004    0.085    4.838    5.172    5.004    6.595
   .x2                4.684    0.177    4.329    5.028    4.684    2.986
   .x3                3.468    0.163    3.146    3.782    3.468    2.385
   .y1                5.267    0.266    4.742    5.769    5.267    2.228
   .y2                4.008    0.408    3.209    4.790    4.008    1.119
   .y3                6.273    0.365    5.562    6.985    6.273    1.954
   .y4                4.191    0.338    3.497    4.815    4.191    1.389
   .y5                4.913    0.304    4.322    5.492    4.913    1.775
   .y6                2.706    0.391    1.943    3.487    2.706    0.763
   .y7                5.892    0.367    5.163    6.583    5.892    1.766
   .y8                3.739    0.386    2.961    4.493    3.739    1.070
    ind60             0.000                               0.000    0.000
   .dem60             0.000                               0.000    0.000
   .dem65             0.000                               0.000    0.000
     Rhat    Prior      
    1.004    normal(3,2)
    1.003    normal(3,2)
    1.001    normal(3,2)
    1.001    normal(3,2)
    1.002    normal(3,2)
    1.001    normal(3,2)
    1.002    normal(3,2)
    1.000    normal(3,2)
    1.001    normal(3,2)
    1.000    normal(3,2)
    1.001    normal(3,2)
                        
                        
                        

Variances:
                   Estimate  Post.SD pi.lower pi.upper   Std.lv  Std.all
   .x1                0.088    0.023    0.049    0.140    0.088    0.154
   .x2                0.129    0.083    0.001    0.306    0.129    0.053
   .x3                0.511    0.097    0.345    0.728    0.511    0.242
   .y1                2.627    0.603    1.615    3.975    2.627    0.470
   .y2                7.275    1.398    4.912   10.378    7.275    0.567
   .y3                5.600    1.108    3.752    8.125    5.600    0.544
   .y4                2.379    0.814    0.915    4.145    2.379    0.261
   .y5                3.180    0.669    2.087    4.715    3.180    0.415
   .y6                4.161    0.862    2.742    6.126    4.161    0.331
   .y7                4.036    0.838    2.573    5.911    4.036    0.362
   .y8                2.044    0.692    0.858    3.585    2.044    0.168
    ind60             1.000                               1.000    1.000
   .dem60             1.000                               0.664    0.664
   .dem65             1.000                               0.440    0.440
     Rhat    Prior      
    1.000 gamma(1,1)[sd]
    0.999 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.001 gamma(1,1)[sd]
    0.999 gamma(1,1)[sd]
    1.001 gamma(1,1)[sd]
    1.001 gamma(1,1)[sd]
    0.999 gamma(1,1)[sd]
    1.000 gamma(1,1)[sd]
    1.001 gamma(1,1)[sd]
    1.001 gamma(1,1)[sd]
                        
                        
                        

R-Square:
                   Estimate
    x1                0.846
    x2                0.947
    x3                0.758
    y1                0.530
    y2                0.433
    y3                0.456
    y4                0.739
    y5                0.585
    y6                0.669
    y7                0.638
    y8                0.832
    dem60             0.336
    dem65             0.560</code></pre>
</div>
</div>
</section>
<section id="model-comparison-11" class="slide level2">
<h2>Model comparison</h2>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-60_6935abbcb000f5ea61368848d547247b">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a href="#cb87-1"></a>bc67 <span class="ot">&lt;-</span> <span class="fu">blavCompare</span>(f6, f7)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
WAIC estimates: 
 object1:  3217.708 
 object2:  3219.098 

 ELPD difference &amp; SE: 
   -0.695    0.681 

LOO estimates: 
 object1:  3218.04 
 object2:  3219.398 

 ELPD difference &amp; SE: 
   -0.679    0.689 

Laplace approximation to the log-Bayes factor
(experimental; positive values favor object1):       NA </code></pre>
</div>
</div>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-61_7cac6874f76d21fc99dd8f6645069bee">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a href="#cb89-1"></a><span class="fu">abs</span>(bc67<span class="sc">$</span>diff_loo[,<span class="st">"elpd_diff"</span>] <span class="sc">/</span> bc67<span class="sc">$</span>diff_loo[,<span class="st">"se_diff"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   model1    model2 
      NaN 0.9858354 </code></pre>
</div>
</div>
</section>
<section id="probability-of-direction" class="slide level2 smaller">
<h2>Probability of direction</h2>
<ul>
<li><span class="math inline">\(pd\)</span> is an index of effect existence (0% to 100%) representing the certainty with which an effect goes in a particular direction (i.e., is positive or negative) <span class="citation" data-cites="makowski_indices_2019">(<a href="#/references" role="doc-biblioref" onclick="">Makowski et al. 2019</a>)</span>.</li>
<li>Beyond its simplicity of interpretation, this index also presents other interesting properties:
<ul>
<li>It is independent from the model: It is solely based on the posterior distributions and does not require any additional information from the data or the model.</li>
<li>It is robust to the scale of both the response variable and the predictors.</li>
<li>It is strongly correlated with the frequentist p-value, and can thus be used to draw parallels and give some reference to readers non-familiar with Bayesian statistics.</li>
</ul></li>
<li>Probability that a parameter (described by its posterior distribution) is above or below a chosen cutoff, an explicit hypothesis.</li>
<li>Although differently expressed, this index is fairly similar (i.e., is strongly correlated) to the frequentist p-value.</li>
</ul>
</section>
<section id="probability-of-direction-1" class="slide level2 smaller">
<h2>Probability of direction</h2>
<ul>
<li>Extract the posterior draws into a matrix</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-62_02b91c4019dfba3439729f6a53b02cec">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a href="#cb91-1"></a>mc_out <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(<span class="fu">blavInspect</span>(f6, <span class="st">"mcmc"</span>, <span class="at">add.labels =</span> <span class="cn">FALSE</span>))</span>
<span id="cb91-2"><a href="#cb91-2"></a><span class="fu">dim</span>(mc_out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3000   40</code></pre>
</div>
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a href="#cb93-1"></a><span class="fu">colnames</span>(mc_out)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "ly_sign[1]"    "ly_sign[2]"    "ly_sign[3]"    "ly_sign[4]"   
 [5] "ly_sign[5]"    "ly_sign[6]"    "ly_sign[7]"    "ly_sign[4]"   
 [9] "ly_sign[5]"    "ly_sign[6]"    "ly_sign[7]"    "bet_sign[1]"  
[13] "bet_sign[2]"   "bet_sign[3]"   "Theta_cov[1]"  "Theta_cov[2]" 
[17] "Theta_cov[3]"  "Theta_cov[4]"  "Theta_var[1]"  "Theta_var[2]" 
[21] "Theta_var[3]"  "Theta_var[4]"  "Theta_var[5]"  "Theta_var[6]" 
[25] "Theta_var[7]"  "Theta_var[8]"  "Theta_var[9]"  "Theta_var[10]"
[29] "Theta_var[11]" "Nu_free[1]"    "Nu_free[2]"    "Nu_free[3]"   
[33] "Nu_free[4]"    "Nu_free[5]"    "Nu_free[6]"    "Nu_free[7]"   
[37] "Nu_free[8]"    "Nu_free[9]"    "Nu_free[10]"   "Nu_free[11]"  </code></pre>
</div>
</div>
</section>
<section id="probability-of-direction-2" class="slide level2 smaller">
<h2>Probability of direction</h2>
<ul>
<li>Find the respective parameter names, and focus on regressions</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-63_eb1db85589dbe35e92ce52165cf40b53">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a href="#cb95-1"></a>pt <span class="ot">&lt;-</span> <span class="fu">partable</span>(f6)[,<span class="fu">c</span>(<span class="st">"lhs"</span>,<span class="st">"op"</span>,<span class="st">"rhs"</span>,<span class="st">"pxnames"</span>)]</span>
<span id="cb95-2"><a href="#cb95-2"></a>pt</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     lhs op   rhs       pxnames
1  ind60 =~    x1    ly_sign[1]
2  ind60 =~    x2    ly_sign[2]
3  ind60 =~    x3    ly_sign[3]
4  dem60 =~    y1    ly_sign[4]
5  dem60 =~    y2    ly_sign[5]
6  dem60 =~    y3    ly_sign[6]
7  dem60 =~    y4    ly_sign[7]
8  dem65 =~    y5    ly_sign[4]
9  dem65 =~    y6    ly_sign[5]
10 dem65 =~    y7    ly_sign[6]
11 dem65 =~    y8    ly_sign[7]
12 dem60  ~ ind60   bet_sign[1]
13 dem65  ~ ind60   bet_sign[2]
14 dem65  ~ dem60   bet_sign[3]
15    y1 ~~    y5  Theta_cov[1]
16    y2 ~~    y6  Theta_cov[2]
17    y3 ~~    y7  Theta_cov[3]
18    y4 ~~    y8  Theta_cov[4]
19    x1 ~~    x1  Theta_var[1]
20    x2 ~~    x2  Theta_var[2]
21    x3 ~~    x3  Theta_var[3]
22    y1 ~~    y1  Theta_var[4]
23    y2 ~~    y2  Theta_var[5]
24    y3 ~~    y3  Theta_var[6]
25    y4 ~~    y4  Theta_var[7]
26    y5 ~~    y5  Theta_var[8]
27    y6 ~~    y6  Theta_var[9]
28    y7 ~~    y7 Theta_var[10]
29    y8 ~~    y8 Theta_var[11]
30 ind60 ~~ ind60          &lt;NA&gt;
31 dem60 ~~ dem60          &lt;NA&gt;
32 dem65 ~~ dem65          &lt;NA&gt;
33    x1 ~1          Nu_free[1]
34    x2 ~1          Nu_free[2]
35    x3 ~1          Nu_free[3]
36    y1 ~1          Nu_free[4]
37    y2 ~1          Nu_free[5]
38    y3 ~1          Nu_free[6]
39    y4 ~1          Nu_free[7]
40    y5 ~1          Nu_free[8]
41    y6 ~1          Nu_free[9]
42    y7 ~1         Nu_free[10]
43    y8 ~1         Nu_free[11]
44 ind60 ~1                &lt;NA&gt;
45 dem60 ~1                &lt;NA&gt;
46 dem65 ~1                &lt;NA&gt;
47  .p4. ==  .p8.          &lt;NA&gt;
48  .p5. ==  .p9.          &lt;NA&gt;
49  .p6. == .p10.          &lt;NA&gt;
50  .p7. == .p11.          &lt;NA&gt;</code></pre>
</div>
</div>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-64_49ca2244a6b5a4880d8bf16bc4e2ea13">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb97-1"><a href="#cb97-1"></a>pt[pt<span class="sc">$</span>op<span class="sc">==</span><span class="st">"~"</span>,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     lhs op   rhs     pxnames
12 dem60  ~ ind60 bet_sign[1]
13 dem65  ~ ind60 bet_sign[2]
14 dem65  ~ dem60 bet_sign[3]</code></pre>
</div>
</div>
</section>
<section id="probability-of-direction-3" class="slide level2 smaller">
<h2>Probability of direction</h2>
<ul>
<li>We will use the function <code>hypothesis()</code> from the package <code>brms</code></li>
<li>Ask specific question of the posterior distributions, for example if we want to know what proportion of the regression <code>dem65~ind60</code> is higher than 0</li>
<li><code>Post.Prob</code> is the pd under the stated hypothesis</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-65_bce89ad01e71834c33e6377f730afd45">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb99-1"><a href="#cb99-1"></a><span class="fu">hypothesis</span>(mc_out, <span class="st">"bet_sign[2] &gt; 0"</span>, <span class="at">alpha =</span> <span class="fl">0.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Hypothesis Tests for class :
         Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob
1 (bet_sign[2]) &gt; 0     0.22      0.17    -0.05     0.49       11.1      0.92
  Star
1     
---
'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
'*': For one-sided hypotheses, the posterior probability exceeds 95%;
for two-sided hypotheses, the value tested against lies outside the 95%-CI.
Posterior probabilities of point hypotheses assume equal prior probabilities.</code></pre>
</div>
</div>
</section>
<section id="probability-of-direction-4" class="slide level2 smaller">
<h2>Probability of direction</h2>
<ul>
<li>In another example, we want to know what proportion of the regression <code>dem60~ind60</code> is higher than 0.</li>
<li>Here we can see that 100% of the posterior probability is higher than 0, in such a case <code>Evid.Ratio = Inf</code>, this will happens when the whole distribution fulfills the hypothesis.</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-66_947d850b24c751749b575276fd5533dc">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb101-1"><a href="#cb101-1"></a><span class="fu">hypothesis</span>(mc_out, <span class="st">"bet_sign[1] &gt; 0"</span>, <span class="at">alpha =</span> <span class="fl">0.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Hypothesis Tests for class :
         Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob
1 (bet_sign[1]) &gt; 0      0.7      0.17     0.44     0.99        Inf         1
  Star
1    *
---
'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
'*': For one-sided hypotheses, the posterior probability exceeds 95%;
for two-sided hypotheses, the value tested against lies outside the 95%-CI.
Posterior probabilities of point hypotheses assume equal prior probabilities.</code></pre>
</div>
</div>
</section>
<section id="probability-of-direction-5" class="slide level2 smaller">
<h2>Probability of direction</h2>
<ul>
<li>Could use this to test equalities between parameters, for example we can test if <code>dem60~ind60</code> is higher than <code>dem65~ind60</code>.</li>
<li>Here we see 98% of the posteriors state that <code>dem60~ind60</code> is higher than <code>dem65~ind60</code>, and the mean of the difference (<code>dem60~ind60 - dem65~ind60</code>) is <code>Estimate=0.48</code></li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-67_de26a841c730615e957bef8cfb17243b">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb103-1"><a href="#cb103-1"></a><span class="fu">hypothesis</span>(mc_out, <span class="st">"bet_sign[1] - bet_sign[2] &gt; 0"</span>, <span class="at">alpha =</span> <span class="fl">0.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Hypothesis Tests for class :
                Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio
1 (bet_sign[1]-bet_... &gt; 0     0.48      0.24      0.1      0.9      51.63
  Post.Prob Star
1      0.98    *
---
'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
'*': For one-sided hypotheses, the posterior probability exceeds 95%;
for two-sided hypotheses, the value tested against lies outside the 95%-CI.
Posterior probabilities of point hypotheses assume equal prior probabilities.</code></pre>
</div>
</div>
</section>
<section id="region-of-practical-equivalence-rope" class="slide level2 smaller">
<h2>Region of Practical Equivalence (ROPE)</h2>
<ul>
<li>Note that so far we have only tested the hypothesis against 0, which would be equivalent to the frequentist null hypothesis tests.</li>
<li>But we can test against any other.</li>
<li>Bayesian inference is not based on statistical significance, where effects are tested against “zero”.</li>
<li>Rather than concluding that an effect is present when it simply differs from zero, we would conclude that the probability of being outside a specific range that can be considered as “practically no effect” (i.e., a negligible magnitude) is sufficient. This range is called the region of practical equivalence (ROPE).</li>
</ul>
</section>
<section id="region-of-practical-equivalence-rope-1" class="slide level2">
<h2>Region of Practical Equivalence (ROPE)</h2>
<ul>
<li>Statistically, the probability of a posterior distribution being different from 0 does not make much sense (the probability of it being different from a single point being infinite). Therefore, the idea underlining ROPE is to let the user define an area around the null value enclosing values that are equivalent to the null value for practical purposes <span class="citation" data-cites="kruschke_bayesian_2018">(<a href="#/references" role="doc-biblioref" onclick="">Kruschke and Liddell 2018</a>)</span></li>
</ul>
</section>
<section id="region-of-practical-equivalence-rope-2" class="slide level2 smaler">
<h2>Region of Practical Equivalence (ROPE)</h2>
<ul>
<li>We would change the value tested, a common recommendations is to use <code>|0.1|</code> as the minimally relevant value for standardized regressions, in this case we find that <code>0.78</code> proportion of the posterior is above <code>0.1</code></li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-68_d16ecb663c762633860b58fdd2412be3">
<div class="sourceCode cell-code" id="cb105"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb105-1"><a href="#cb105-1"></a><span class="fu">hypothesis</span>(mc_out, <span class="st">"bet_sign[2] &gt; .1"</span>, <span class="at">alpha =</span> <span class="fl">0.05</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Hypothesis Tests for class :
              Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio
1 (bet_sign[2])-(.1) &gt; 0     0.12      0.17    -0.15     0.39       3.49
  Post.Prob Star
1      0.78     
---
'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.
'*': For one-sided hypotheses, the posterior probability exceeds 95%;
for two-sided hypotheses, the value tested against lies outside the 95%-CI.
Posterior probabilities of point hypotheses assume equal prior probabilities.</code></pre>
</div>
</div>
</section>
<section id="vs.-95-ci" class="slide level2 smaller">
<h2>89% vs.&nbsp;95% CI</h2>
<ul>
<li>Commonly and from the frequentist tradition you will see the use of the 95% Credible interval.</li>
<li>Naturally, when it came about choosing the CI level to report by default, people started using 95%, the arbitrary convention used in the frequentist world.</li>
<li>However, some authors suggested that 95% might not be the most appropriate for Bayesian posterior distributions, potentially lacking stability if not enough posterior samples are drawn <span class="citation" data-cites="mcelreath_statistical_2020">(<a href="#/references" role="doc-biblioref" onclick="">McElreath 2020</a>)</span>.</li>
<li>The proposition was to use 90% instead of 95%. However, recently, <span class="citation" data-cites="mcelreath_statistical_2020">McElreath (<a href="#/references" role="doc-biblioref" onclick="">2020</a>)</span> suggested that if we were to use arbitrary thresholds in the first place, why not use 89%?</li>
<li>89 is the highest prime number that does not exceed the already unstable 95% threshold. What does it have to do with anything? Nothing, but it reminds us of the total arbitrariness of these conventions <span class="citation" data-cites="mcelreath_statistical_2020">(<a href="#/references" role="doc-biblioref" onclick="">McElreath 2020</a>)</span>.</li>
<li>You can use this as the argument <code>alpha</code> argument in the <code>hypothesis</code> function, or as the interpretation values for <code>Post.Prob</code></li>
</ul>
</section>
<section id="caveats" class="slide level2">
<h2>Caveats</h2>
<ul>
<li>Although this allows testing of hypotheses in a similar manner as in the frequentist null-hypothesis testing framework, we strongly argue against using arbitrary cutoffs (e.g., <span class="math inline">\(p &lt; .05\)</span>) to determine the ‘existence’ of an effect.</li>
<li>ROPE is sensitive to scale, so be aware that the value of interest is representative in the respective scale. For this, standardize parameters are useful to have in a commonly used scale</li>
</ul>
</section></section>
<section>
<section id="local-fit" class="title-slide slide level1 center">
<h1>Local fit</h1>

</section>
<section id="local-fit-1" class="slide level2">
<h2>Local fit</h2>
<ul>
<li>Evaluates <em>local fit</em>, misfit related to a specific part of the model (like failing to reproduce a pairwise correlation)</li>
<li>Residual correlations can be used to <em>diagnose</em> the problem, but doesnt indicate how to fix it</li>
<li>Modification indices <span class="citation" data-cites="whittaker_using_2012">(<a href="#/references" role="doc-biblioref" onclick="">Whittaker 2012</a>)</span>
<ul>
<li>Lagrange approximation of the expected improvement in model fit (<span class="math inline">\(\chi^2\)</span>) if a parameter is added (MI)</li>
<li>Approximate the standardized expected parameter change (SEPC), <em>what would the standardized parameter be?</em></li>
<li>Use to test the worth of <strong>adding</strong> a parameter that was not included in the original model</li>
</ul></li>
</ul>
</section>
<section id="bayesian-modification-indices" class="slide level2">
<h2>Bayesian Modification Indices</h2>
<ul>
<li>We are able to estimate their Bayesian version with PPMC</li>
<li>PPMC: Posterior Predictive Model checks
<ul>
<li>Post calculate <em>something</em> at each draw of the posterior distribution</li>
<li>Build posterior distributions of the new calculation</li>
<li>We can do this for MI and SEPC</li>
</ul></li>
<li>Changes to the model should be <strong>theoretically</strong> defensible</li>
</ul>
</section>
<section id="bayesian-modification-indices-1" class="slide level2">
<h2>Bayesian Modification Indices</h2>
<ul>
<li>Basic measurement model (weakly informative priors)</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-69_94f5e8c4bf13c73c139ce13e20db1dc7">
<div class="sourceCode cell-code" id="cb107"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb107-1"><a href="#cb107-1"></a><span class="fu">summary</span>(f2, <span class="at">rsquare=</span>T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>blavaan 0.5.3 ended normally after 1000 iterations

  Estimator                                      BAYES
  Optimization method                             MCMC
  Number of model parameters                        36

  Number of observations                            75

  Statistic                                 MargLogLik         PPP
  Value                                      -1667.792       0.040

Parameter Estimates:


Latent Variables:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior      
  ind60 =~                                                                    
    x1                0.698    0.070    0.572    0.844    1.000   normal(1, 3)
    x2                1.532    0.137    1.291    1.824    1.000   normal(1, 3)
    x3                1.265    0.141    1.003    1.552    0.999   normal(1, 3)
  dem60 =~                                                                    
    y1                2.266    0.260    1.798    2.810    0.999   normal(1, 3)
    y2                3.060    0.398    2.298    3.871    1.000   normal(1, 3)
    y3                2.379    0.353    1.713    3.079    1.000   normal(1, 3)
    y4                2.964    0.317    2.384    3.639    1.000   normal(1, 3)
  dem65 =~                                                                    
    y5                2.133    0.263    1.663    2.696    1.000   normal(1, 3)
    y6                2.715    0.342    2.082    3.385    1.002   normal(1, 3)
    y7                2.754    0.325    2.150    3.421    1.002   normal(1, 3)
    y8                2.836    0.312    2.280    3.488    1.001   normal(1, 3)

Covariances:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior      
  ind60 ~~                                                                    
    dem60             0.426    0.105    0.209    0.616    1.002    lkj_corr(1)
    dem65             0.531    0.094    0.335    0.695    1.001    lkj_corr(1)
  dem60 ~~                                                                    
    dem65             0.952    0.030    0.879    0.993    1.002    lkj_corr(1)

Intercepts:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior      
   .x1                5.011    0.088    4.848    5.184    1.000    normal(3,2)
   .x2                4.697    0.178    4.361    5.045    1.001    normal(3,2)
   .x3                3.479    0.167    3.161    3.821    1.001    normal(3,2)
   .y1                5.191    0.293    4.610    5.764    1.001    normal(3,2)
   .y2                3.888    0.431    3.056    4.731    1.000    normal(3,2)
   .y3                6.230    0.377    5.493    6.979    1.000    normal(3,2)
   .y4                4.102    0.375    3.330    4.811    1.001    normal(3,2)
   .y5                4.872    0.290    4.271    5.419    1.001    normal(3,2)
   .y6                2.666    0.370    1.916    3.399    1.001    normal(3,2)
   .y7                5.843    0.364    5.143    6.546    1.001    normal(3,2)
   .y8                3.708    0.360    3.011    4.419    1.001    normal(3,2)
    ind60             0.000                                                   
    dem60             0.000                                                   
    dem65             0.000                                                   

Variances:
                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior      
   .x1                0.091    0.022    0.051    0.137    1.000 gamma(1,1)[sd]
   .x2                0.112    0.077    0.001    0.283    1.001 gamma(1,1)[sd]
   .x3                0.513    0.103    0.341    0.736    0.999 gamma(1,1)[sd]
   .y1                2.072    0.461    1.306    3.083    1.000 gamma(1,1)[sd]
   .y2                6.683    1.267    4.607    9.485    1.000 gamma(1,1)[sd]
   .y3                5.514    1.014    3.835    7.789    1.001 gamma(1,1)[sd]
   .y4                2.915    0.686    1.805    4.420    1.000 gamma(1,1)[sd]
   .y5                2.560    0.503    1.743    3.671    1.000 gamma(1,1)[sd]
   .y6                4.430    0.840    3.003    6.271    1.000 gamma(1,1)[sd]
   .y7                3.646    0.746    2.416    5.330    0.999 gamma(1,1)[sd]
   .y8                2.956    0.646    1.871    4.453    1.000 gamma(1,1)[sd]
    ind60             1.000                                                   
    dem60             1.000                                                   
    dem65             1.000                                                   

R-Square:
                   Estimate
    x1                0.842
    x2                0.955
    x3                0.757
    y1                0.713
    y2                0.584
    y3                0.507
    y4                0.751
    y5                0.640
    y6                0.625
    y7                0.675
    y8                0.731</code></pre>
</div>
</div>
</section>
<section id="bayesian-modification-indices-2" class="slide level2">
<h2>Bayesian Modification Indices</h2>
<ul>
<li>Write a function to do the desire post calculation</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-70_cc5b8e8931bbd4b916bd77b3924faa86">
<div class="sourceCode cell-code" id="cb109"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb109-1"><a href="#cb109-1"></a>discFUN <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb109-2"><a href="#cb109-2"></a>  <span class="at">mod.ind_mi =</span> <span class="cf">function</span>(object){</span>
<span id="cb109-3"><a href="#cb109-3"></a>    temp <span class="ot">&lt;-</span> <span class="fu">modificationindices</span>(object, <span class="at">free.remove =</span> F)</span>
<span id="cb109-4"><a href="#cb109-4"></a>    mods <span class="ot">&lt;-</span> temp<span class="sc">$</span>mi</span>
<span id="cb109-5"><a href="#cb109-5"></a>    <span class="fu">names</span>(mods) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(temp<span class="sc">$</span>lhs, temp<span class="sc">$</span>op, temp<span class="sc">$</span>rhs)</span>
<span id="cb109-6"><a href="#cb109-6"></a>    <span class="fu">return</span>(mods)</span>
<span id="cb109-7"><a href="#cb109-7"></a>  },</span>
<span id="cb109-8"><a href="#cb109-8"></a>  <span class="at">mod.ind_sepc.all =</span> <span class="cf">function</span>(object){</span>
<span id="cb109-9"><a href="#cb109-9"></a>    temp <span class="ot">&lt;-</span> <span class="fu">modificationindices</span>(object, <span class="at">free.remove =</span> F)</span>
<span id="cb109-10"><a href="#cb109-10"></a>    sepc.all <span class="ot">&lt;-</span> temp<span class="sc">$</span>sepc.all</span>
<span id="cb109-11"><a href="#cb109-11"></a>    <span class="fu">names</span>(sepc.all) <span class="ot">&lt;-</span> <span class="fu">paste0</span>(temp<span class="sc">$</span>lhs, temp<span class="sc">$</span>op, temp<span class="sc">$</span>rhs)</span>
<span id="cb109-12"><a href="#cb109-12"></a>    <span class="fu">return</span>(sepc.all)</span>
<span id="cb109-13"><a href="#cb109-13"></a>  }</span>
<span id="cb109-14"><a href="#cb109-14"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="bayesian-modification-indices-3" class="slide level2">
<h2>Bayesian Modification Indices</h2>
<ul>
<li>Pass this function to the <code>ppmc()</code> function of <code>blavaan</code>. With this function, the MI and SEPC are computed for each posterior sample, leading to posterior distributions for each of them</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-71_90872a32eb0daa0329d9728ac51657cd">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb110-1"><a href="#cb110-1"></a>out <span class="ot">&lt;-</span> <span class="fu">ppmc</span>(f2, <span class="at">discFUN =</span> discFUN)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="bayesian-modification-indices-4" class="slide level2">
<h2>Bayesian Modification Indices</h2>
<ul>
<li>See the top 5 parameters to add in function of MI mean</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-72_1d6024fd6d521f078c163c351f5fa537">
<div class="sourceCode cell-code" id="cb111"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb111-1"><a href="#cb111-1"></a><span class="fu">summary</span>(out, <span class="at">prob=</span>.<span class="dv">9</span>, <span class="at">discFUN =</span> <span class="st">"mod.ind_mi"</span>, <span class="at">sort.by=</span><span class="st">"EAP"</span>, <span class="at">decreasing=</span>T)[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Posterior summary statistics and highest posterior density (HPD) 90% credible intervals for the posterior distribution of realized discrepancy-function values based on observed data, along with posterior predictive p values to test hypotheses in either direction:

              EAP Median     MAP       SD lower   upper PPP_sim_GreaterThan_obs
dem65=~y2 120.569 13.466 -17.640 1697.643     0 130.957                   0.526
dem65=~y4 113.778 21.434  42.035  776.792     0 185.653                   0.502
dem65=~y1 113.024 19.941  38.444  805.963     0 177.861                   0.496
dem60=~y7  98.863 15.330  55.900  825.815     0 150.508                   0.507
dem60=~y8  96.228 16.733  51.812  772.921     0 148.431                   0.509
          PPP_sim_LessThan_obs
dem65=~y2                0.474
dem65=~y4                0.498
dem65=~y1                0.504
dem60=~y7                0.493
dem60=~y8                0.491</code></pre>
</div>
</div>
</section>
<section id="bayesian-modification-indices-5" class="slide level2">
<h2>Bayesian Modification Indices</h2>
<ul>
<li>See the top 5 parameters to add in function of MI median</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-73_ce39516a62291acaf0d1e9cdf50c97de">
<div class="sourceCode cell-code" id="cb113"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb113-1"><a href="#cb113-1"></a><span class="fu">summary</span>(out, <span class="at">prob=</span>.<span class="dv">9</span>, <span class="at">discFUN =</span> <span class="st">"mod.ind_mi"</span>, <span class="at">sort.by=</span><span class="st">"Median"</span>, <span class="at">decreasing=</span>T)[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Posterior summary statistics and highest posterior density (HPD) 90% credible intervals for the posterior distribution of realized discrepancy-function values based on observed data, along with posterior predictive p values to test hypotheses in either direction:

              EAP Median    MAP      SD lower   upper PPP_sim_GreaterThan_obs
dem65=~y4 113.778 21.434 42.035 776.792     0 185.653                   0.502
dem65=~y1 113.024 19.941 38.444 805.963     0 177.861                   0.496
x1~~x2     66.639 17.381  3.083 164.962     0 156.441                   0.513
dem60=~y8  96.228 16.733 51.812 772.921     0 148.431                   0.509
dem60=~y6  90.425 15.547 31.690 619.905     0 136.963                   0.502
          PPP_sim_LessThan_obs
dem65=~y4                0.498
dem65=~y1                0.504
x1~~x2                   0.487
dem60=~y8                0.491
dem60=~y6                0.498</code></pre>
</div>
</div>
</section>
<section id="bayesian-modification-indices-6" class="slide level2">
<h2>Bayesian Modification Indices</h2>
<ul>
<li>See the top 5 parameters to add in function of SEPC mean</li>
</ul>
<div class="cell" data-hash="BSEM_APS_2023_cache/revealjs/unnamed-chunk-74_4123c4af389cadf63b9456cccbead9ba">
<div class="sourceCode cell-code" id="cb115"><pre class="sourceCode numberSource r number-lines code-with-copy"><code class="sourceCode r"><span id="cb115-1"><a href="#cb115-1"></a><span class="fu">summary</span>(out, <span class="at">prob=</span>.<span class="dv">9</span>, <span class="at">discFUN =</span> <span class="st">"mod.ind_sepc.all"</span>, <span class="at">sort.by=</span><span class="st">"EAP"</span>, <span class="at">decreasing=</span>T)[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Posterior summary statistics and highest posterior density (HPD) 90% credible intervals for the posterior distribution of realized discrepancy-function values based on observed data, along with posterior predictive p values to test hypotheses in either direction:

            EAP Median    MAP     SD  lower  upper PPP_sim_GreaterThan_obs
dem65=~y2 1.567  0.032  2.085 73.661 -7.845  8.074                   0.506
dem65=~y4 1.486  0.622  2.891 49.866 -9.085 10.584                   0.435
dem60=~y5 1.127  0.187  2.684 37.861 -7.491  8.183                   0.486
x2~~y2    0.739  0.157  0.382 11.935 -0.014  0.496                      NA
x2~~y3    0.589  0.112 -0.161  9.246 -0.014  0.385                      NA
          PPP_sim_LessThan_obs
dem65=~y2                0.494
dem65=~y4                0.565
dem60=~y5                0.514
x2~~y2                      NA
x2~~y3                      NA</code></pre>
</div>
</div>
</section>
<section id="bayesian-modification-indices-7" class="slide level2">
<h2>Bayesian Modification Indices</h2>
<ul>
<li>We tested them with a simulation, paper under review</li>
<li>Use MI to identify the <em>best</em> parameters to add</li>
<li>Use SEPC to approximate the effect size of the respective parameters</li>
</ul>
</section></section>
<section>
<section id="what-to-report" class="title-slide slide level1 center">
<h1>What to report</h1>

</section>
<section id="what-to-report-1" class="slide level2 smaller">
<h2>What to report</h2>
<ul>
<li>Process
<ul>
<li>What steps where taken in the model building process</li>
</ul></li>
<li>Priors
<ul>
<li>Specify which priors you choose</li>
<li>Prior sensitivity</li>
</ul></li>
<li>Parameters: present point estimate and uncertainty
<ul>
<li>Mean and/or median</li>
<li>SD and/or CI</li>
<li>“<strong>Given the observed data</strong>, we are 95% confident that the latent regression falls between X and Y”</li>
</ul></li>
</ul>
</section>
<section id="what-to-report-2" class="slide level2 smaller">
<h2>What to report</h2>
<ul>
<li><span class="math inline">\(pd\)</span>/ROPE
<ul>
<li>Which hypotheses were tested</li>
<li>Which values were used for ROPE and why</li>
</ul></li>
<li>Model fit
<ul>
<li>CFI and <span class="math inline">\(\hat{\Gamma}\)</span> point estimate and uncertainty</li>
<li>Changes made in function of local misfit (like modification indices)</li>
</ul></li>
<li>Model comparison
<ul>
<li>LOO or WAIC: IC for each model, <span class="math inline">\(elpd\)</span>, <span class="math inline">\(\Delta elpd\)</span>, standard error, and ratio</li>
</ul></li>
</ul>
</section>
<section id="resources" class="slide level2 smaller">
<h2>Resources</h2>
<ul>
<li><a href="https://www.statscamp.org/">Stats Camp</a></li>
<li>Online
<ul>
<li><a href="http://ecmerkle.github.io/blavaan/">blavaan website</a></li>
<li><a href="https://groups.google.com/g/blavaan">blavaan google group</a></li>
<li><a href="https://groups.google.com/g/lavaan">lavaan google group</a></li>
<li><a href="https://discourse.mc-stan.org/">Stan discourse</a></li>
<li><a href="https://github.com/maugavilla/well_hello_stats">Well Hello Stats</a></li>
</ul></li>
<li>Books
<ul>
<li><a href="http://www.stat.columbia.edu/~gelman/book/">Bayesian Data Analysis, 3th edition</a></li>
<li><a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking, 2nd edition</a></li>
<li><a href="https://www.guilford.com/books/Bayesian-Statistics-for-the-Social-Sciences/David-Kaplan/9781462516513">Bayesian Statistics for the Social Sciences</a></li>
<li><a href="https://www.guilford.com/books/Bayesian-Structural-Equation-Modeling/Sarah-Depaoli/9781462547746">Bayesian Structural Equation Modeling</a></li>
<li><a href="https://www.guilford.com/books/Longitudinal-Structural-Equation-Modeling/Todd-Little/9781462553143">Longitudinal Structural Equation Modeling, 2nd edition</a></li>
</ul></li>
</ul>
</section>
<section id="contact" class="slide level2">
<h2>Contact</h2>
<p>m.garniervillarreal@vu.nl</p>
<p><a href="https://maugavilla.github.io/">Website</a></p>
<p>@MauricioGarnier</p>
<ul>
<li>Funding: Institute of Education Sciences (IES), Statistical and Research Methodology in Education. Merkle, E. C (PI), Bonifay, W., Garnier-Villarreal, M., &amp; Rosseel, Y. Scaling Bayesian Latent Variable Models to Big Education Data.</li>
</ul>
</section>
<section id="references" class="slide level2 smaller scrollable">
<h2>References</h2>

<img src="VU_social_avatar_blauw.png" class="slide-logo r-stretch"><div class="footer footer-default">

</div>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-bollen_structural_1989" class="csl-entry" role="listitem">
Bollen, Kenneth A. 1989. <em>Structural <span>Equations</span> with <span>Latent</span> <span>Variables</span></em>. Wiley Series in Probability and Mathematical Statistics. John Wiley &amp; Sons, Inc.
</div>
<div id="ref-Gabry_2019_vis" class="csl-entry" role="listitem">
Gabry, Jonah, Daniel Simpson, Aki Vehtari, Michael Betancourt, and Andrew Gelman. 2019. <span>“<span class="nocase">Visualization in Bayesian Workflow</span>.”</span> <em>Journal of the Royal Statistical Society Series A: Statistics in Society</em> 182 (2): 389–402. <a href="https://doi.org/10.1111/rssa.12378">https://doi.org/10.1111/rssa.12378</a>.
</div>
<div id="ref-garnier_adapting_2020" class="csl-entry" role="listitem">
Garnier-Villarreal, Mauricio, and Terrence D Jorgensen. 2020. <span>“Adapting <span>Fit</span> <span>Indices</span> for <span>Bayesian</span> <span>Structural</span> <span>Equation</span> <span>Modeling</span>: <span>Comparison</span> to <span>Maximum</span> <span>Likelihood</span>.”</span> <em>Psychological Methods</em> 25 (1): 46–70. <a href="https://doi.org/dx.doi.org/10.1037/met0000224">https://doi.org/dx.doi.org/10.1037/met0000224</a>.
</div>
<div id="ref-kruschke_bayesian_2018" class="csl-entry" role="listitem">
Kruschke, John K., and Torrin M. Liddell. 2018. <span>“The <span>Bayesian</span> <span>New</span> <span>Statistics</span>: <span>Hypothesis</span> Testing, Estimation, Meta-Analysis, and Power Analysis from a <span>Bayesian</span> Perspective.”</span> <em>Psychonomic Bulletin &amp; Review</em> 25 (1): 178–206. <a href="https://doi.org/10.3758/s13423-016-1221-4">https://doi.org/10.3758/s13423-016-1221-4</a>.
</div>
<div id="ref-makowski_indices_2019" class="csl-entry" role="listitem">
Makowski, Dominique, Mattan S. Ben-Shachar, S. H. Annabel Chen, and Daniel Lüdecke. 2019. <span>“Indices of <span>Effect</span> <span>Existence</span> and <span>Significance</span> in the <span>Bayesian</span> <span>Framework</span>.”</span> <em>Frontiers in Psychology</em> 10 (December): 2767. <a href="https://doi.org/10.3389/fpsyg.2019.02767">https://doi.org/10.3389/fpsyg.2019.02767</a>.
</div>
<div id="ref-mcelreath_statistical_2020" class="csl-entry" role="listitem">
McElreath, Richard. 2020. <em>Statistical Rethinking: A <span>Bayesian</span> Course with Examples in <span>R</span> and <span>Stan</span></em>. 2nd ed. <span>CRC</span> Texts in Statistical Science. Boca Raton: Taylor; Francis, CRC Press.
</div>
<div id="ref-merkle2023opaque" class="csl-entry" role="listitem">
Merkle, Edgar C., Oludare Ariyo, Sonja D. Winter, and Mauricio Garnier-Villarreal. 2023. <span>“Opaque Prior Distributions in Bayesian Latent Variable Models.”</span> <a href="https://arxiv.org/abs/2301.08667">https://arxiv.org/abs/2301.08667</a>.
</div>
<div id="ref-schad_workflow_2022" class="csl-entry" role="listitem">
Schad, Daniel J., Bruno Nicenboim, Paul-Christian Bürkner, Michael Betancourt, and Shravan Vasishth. 2022. <span>“Workflow Techniques for the Robust Use of Bayes Factors.”</span> <em>Psychological Methods</em>, March. <a href="https://doi.org/10.1037/met0000472">https://doi.org/10.1037/met0000472</a>.
</div>
<div id="ref-tendeiro_review_2019" class="csl-entry" role="listitem">
Tendeiro, Jorge N., and Henk A. L. Kiers. 2019. <span>“A Review of Issues about Null Hypothesis <span>Bayesian</span> Testing.”</span> <em>Psychological Methods</em> 24 (6): 774–95. <a href="https://doi.org/10.1037/met0000221">https://doi.org/10.1037/met0000221</a>.
</div>
<div id="ref-vehtari_practical_2017" class="csl-entry" role="listitem">
Vehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017. <span>“Practical <span>Bayesian</span> Model Evaluation Using Leave-One-Out Cross-Validation and <span>WAIC</span>.”</span> <em>Statistics and Computing</em> 27 (5): 1413–32. <a href="https://doi.org/10.1007/s11222-016-9696-4">https://doi.org/10.1007/s11222-016-9696-4</a>.
</div>
<div id="ref-new_rhat" class="csl-entry" role="listitem">
Vehtari, Aki, Andrew Gelman, Daniel Simpson, Bob Carpenter, and Paul-Christian Bürkner. 2021. <span>“<span class="nocase">Rank-Normalization, Folding, and Localization: An Improved <span class="math inline">\(\widehat{R}\)</span> for Assessing Convergence of MCMC (with Discussion)</span>.”</span> <em>Bayesian Analysis</em> 16 (2): 667–718. <a href="https://doi.org/10.1214/20-BA1221">https://doi.org/10.1214/20-BA1221</a>.
</div>
<div id="ref-watanabeAsymptoticEquivalenceBayesa" class="csl-entry" role="listitem">
Watanabe, Sumio. 2010. <span>“Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory.”</span> <em>Journal of Machine Learning Research</em> 11: 3571–94.
</div>
<div id="ref-whittaker_using_2012" class="csl-entry" role="listitem">
Whittaker, Tiffany A. 2012. <span>“Using the <span>Modification</span> <span>Index</span> and <span>Standardized</span> <span>Expected</span> <span>Parameter</span> <span>Change</span> for <span>Model</span> <span>Modification</span>.”</span> <em>The Journal of Experimental Education</em> 80 (1): 26–44. <a href="https://doi.org/10.1080/00220973.2010.531299">https://doi.org/10.1080/00220973.2010.531299</a>.
</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="BSEM_APS_2023_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="BSEM_APS_2023_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="BSEM_APS_2023_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="BSEM_APS_2023_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="BSEM_APS_2023_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="BSEM_APS_2023_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="BSEM_APS_2023_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="BSEM_APS_2023_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="BSEM_APS_2023_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="BSEM_APS_2023_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        // dispatch for htmlwidgets
        function fireSlideEnter() {
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);
        }

        function fireSlideChanged(previousSlide, currentSlide) {
          fireSlideEnter();

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>